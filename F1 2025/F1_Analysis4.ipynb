{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 1 - Setup Environment and Import Libraries\n",
    "import os\n",
    "import fastf1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(os.getcwd(), \"analysis.log\"),\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "BASE_DIR = os.getcwd()  # Simplified for Jupyter\n",
    "CACHE_DIR = os.getenv(\"FASTF1_CACHE_DIR\", os.path.join(BASE_DIR, \"fastf1_cache\"))\n",
    "HISTORICAL_CACHE_DIR = os.getenv(\"HISTORICAL_CACHE_DIR\", os.path.join(BASE_DIR, \"historical_cache\"))\n",
    "try:\n",
    "    os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "    os.makedirs(HISTORICAL_CACHE_DIR, exist_ok=True)\n",
    "except OSError as e:\n",
    "    logger.error(f\"Failed to create cache directory at {CACHE_DIR} or {HISTORICAL_CACHE_DIR}: {e}\")\n",
    "    raise RuntimeError(f\"Failed to create cache directory: {e}\")\n",
    "\n",
    "fastf1.set_log_level('ERROR')\n",
    "fastf1.Cache.enable_cache(CACHE_DIR)\n",
    "\n",
    "if os.getenv(\"SILENT\", \"false\").lower() != \"true\":\n",
    "    print(\"Library Versions:\")\n",
    "    print(f\"FastF1: {fastf1.__version__}\")\n",
    "    print(f\"Pandas: {pd.__version__}\")\n",
    "    print(f\"NumPy: {np.__version__}\")\n",
    "    print(f\"Matplotlib: {plt.matplotlib.__version__}\")\n",
    "    print(f\"\\nCache Directory: {os.path.abspath(CACHE_DIR)}\")\n",
    "    print(f\"Historical Cache Directory: {os.path.abspath(HISTORICAL_CACHE_DIR)}\")\n",
    "    print(f\"Cache Directory Exists: {os.path.exists(CACHE_DIR)}\")\n",
    "    print(f\"Historical Cache Directory Exists: {os.path.exists(HISTORICAL_CACHE_DIR)}\")\n",
    "    print(f\"Current Date: {datetime.now().strftime('%Y-%m-%d')}\")\n",
    "    logger.info(\"Environment setup completed successfully.\")\n",
    "\n",
    "# Why: Sets up the environment with necessary libraries and caching for F1 data analysis.\n",
    "# How to Read: Displays library versions and cache status to confirm setup.\n",
    "# Expected Inputs: None\n",
    "# Expected Outputs: Console confirmation of setup\n",
    "# Data Exported: analysis.log (optional log file)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 2 - Determine the Target Race and Sessions\n",
    "# Objective: Dynamically determine the target race, its session types, historical performance, and track metadata.\n",
    "\n",
    "import fastf1\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from IPython.display import display\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_target_event(manual_year=None, manual_round=None, skip_timing_validation=False):\n",
    "    current_date = datetime.utcnow()\n",
    "    year = manual_year if manual_year is not None else current_date.year\n",
    "    schedule = fastf1.get_event_schedule(year)\n",
    "    print(f\"Total Events in {year} Schedule: {len(schedule)}\")\n",
    "    logger.info(f\"Fetched {year} schedule with {len(schedule)} events.\")\n",
    "\n",
    "    if manual_round is not None and manual_year is not None:\n",
    "        target_event = schedule[schedule['RoundNumber'] == manual_round].iloc[0]\n",
    "    else:\n",
    "        future_events = schedule[schedule['EventDate'] >= current_date - timedelta(days=7)]\n",
    "        target_event = future_events.iloc[0] if not future_events.empty else schedule.iloc[-1]\n",
    "\n",
    "    round_number = target_event['RoundNumber']\n",
    "    event_name = target_event['EventName']\n",
    "    event_date = target_event['EventDate'].strftime('%Y-%m-%d')\n",
    "    is_sprint = 'sprint' in target_event['EventFormat'].lower()\n",
    "    location = target_event['Location']\n",
    "\n",
    "    session_types = []\n",
    "    session_dates = {}\n",
    "    for session in ['Session1', 'Session2', 'Session3']:\n",
    "        session_name = target_event[session]\n",
    "        if session_name in ['Practice 1', 'Practice 2', 'Practice 3', 'Sprint Qualifying', 'Sprint']:\n",
    "            session_types.append(session_name)\n",
    "            session_dates[session_name] = target_event[f\"{session}DateUtc\"]\n",
    "    if not session_types or session_types[-1] not in ['Practice 3', 'Sprint']:\n",
    "        logger.error(f\"No valid final session found for {event_name}\")\n",
    "        raise ValueError(f\"No valid final session found for {event_name}\")\n",
    "\n",
    "    if not skip_timing_validation:\n",
    "        final_session_name = session_types[-1]\n",
    "        final_session_end = session_dates[final_session_name]\n",
    "        qualifying_date_key = 'Session4DateUtc' if not is_sprint else 'Session5DateUtc'\n",
    "        qualifying_start = target_event[qualifying_date_key] if qualifying_date_key in target_event else None\n",
    "        if final_session_end and current_date < final_session_end:\n",
    "            logger.warning(f\"Current time {current_date} is before {final_session_name} end {final_session_end}.\")\n",
    "            raise ValueError(f\"Analysis must run after {final_session_name}.\")\n",
    "        if qualifying_start and current_date > qualifying_start:\n",
    "            logger.warning(f\"Current time {current_date} is after Qualifying start {qualifying_start}.\")\n",
    "            raise ValueError(\"Analysis must run before Qualifying starts.\")\n",
    "\n",
    "    historical_winners = []\n",
    "    historical_poles = []\n",
    "    try:\n",
    "        for past_year in range(year-3, year):\n",
    "            past_schedule = fastf1.get_event_schedule(past_year)\n",
    "            past_event = past_schedule[past_schedule['Location'] == location]\n",
    "            if not past_event.empty:\n",
    "                past_event = past_event.iloc[0]\n",
    "                past_race = fastf1.get_session(past_year, past_event['RoundNumber'], 'R')\n",
    "                past_race.load()\n",
    "                past_qualifying = fastf1.get_session(past_year, past_event['RoundNumber'], 'Q')\n",
    "                past_qualifying.load()\n",
    "                winner = past_race.results.iloc[0]['Abbreviation']\n",
    "                pole_sitter = past_qualifying.results.iloc[0]['Abbreviation']\n",
    "                weight = 3 - (year - past_year)\n",
    "                historical_winners.append((past_year, winner, weight))\n",
    "                historical_poles.append((past_year, pole_sitter, weight))\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to fetch historical data for {location}: {e}\")\n",
    "        historical_winners = [(\"N/A\", \"Unknown\", 1)]\n",
    "        historical_poles = [(\"N/A\", \"Unknown\", 1)]\n",
    "\n",
    "    # Dynamic track metadata (placeholders; computed in a later cell)\n",
    "    track_length = None  # From telemetry (e.g., FP1)\n",
    "    avg_speed = None     # From lap times (e.g., FP1)\n",
    "    downforce_level = None  # To be inferred from telemetry (e.g., corner count vs. straight speed)\n",
    "\n",
    "    event_details = pd.DataFrame({\n",
    "        'Round': [round_number],\n",
    "        'Event Name': [event_name],\n",
    "        'Event Date': [event_date],\n",
    "        'Location': [location],\n",
    "        'Sprint Weekend': [is_sprint],\n",
    "        'Sessions': [', '.join(session_types)],\n",
    "        'Track Length (km)': [track_length if track_length else 'TBD'],\n",
    "        'Avg Speed (km/h)': [avg_speed if avg_speed else 'TBD'],\n",
    "        'Downforce Level': [downforce_level if downforce_level else 'TBD']\n",
    "    })\n",
    "\n",
    "    historical_winners_df = pd.DataFrame(historical_winners, columns=['Year', 'Winner', 'Weight'])\n",
    "    historical_poles_df = pd.DataFrame(historical_poles, columns=['Year', 'Pole Sitter', 'Weight'])\n",
    "\n",
    "    print(\"\\nTarget Event Details (Track data TBD until session loaded):\")\n",
    "    display(event_details)\n",
    "    print(\"\\nHistorical Winners at this Track:\")\n",
    "    display(historical_winners_df)\n",
    "    print(\"\\nHistorical Pole Sitters at this Track:\")\n",
    "    display(historical_poles_df)\n",
    "\n",
    "    if os.getenv(\"SILENT\", \"false\").lower() != \"true\":\n",
    "        print(\"\\nFull Event Row from Schedule:\")\n",
    "        print(target_event)\n",
    "\n",
    "    result = {\n",
    "        'year': year,\n",
    "        'round': round_number,\n",
    "        'session_types': session_types,\n",
    "        'event_name': event_name,\n",
    "        'event_date': event_date,\n",
    "        'location': location,\n",
    "        'historical_winners': historical_winners,\n",
    "        'historical_poles': historical_poles,\n",
    "        'track_length': track_length,\n",
    "        'avg_speed': avg_speed,\n",
    "        'downforce_level': downforce_level\n",
    "    }\n",
    "    print(\"Debug - Step 2 result:\", result)\n",
    "    logger.info(f\"Target event selected: {event_name} (Round {round_number}) with sessions {session_types}\")\n",
    "    return result\n",
    "\n",
    "target_info = get_target_event(manual_year=2024, manual_round=3, skip_timing_validation=True)\n",
    "\n",
    "# Why: Identifies the target race and sessions for pre-qualifying analysis with historical and track context.\n",
    "# How to Read: Displays the selected race, its sessions, historical performance with weights, and track metadata (TBD until computed) to inform predictions.\n",
    "# Expected Inputs: Optional manual_year, manual_round, skip_timing_validation\n",
    "# Expected Outputs: Event schedule length, event details table (track data TBD), historical data with weights, full event row, structured result\n",
    "# Data Exported: Dictionary with race and session details, track data placeholders"
   ],
   "id": "ec7516c0c728e5dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 3 - Download Practice Session Data\n",
    "# Objective: Retrieve data for all practice sessions, extract key indicators, compute track metadata, save as CSV, and preview for verification.\n",
    "\n",
    "import fastf1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import display\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def download_practice_data(target_info):\n",
    "    year = target_info['year']\n",
    "    round_number = target_info['round']\n",
    "    session_labels = target_info['session_types']\n",
    "    event_name = target_info['event_name']\n",
    "\n",
    "    # Directory to save raw data (structured as raw_data/{year}/R{round_number}/)\n",
    "    BASE_DIR = os.getcwd()\n",
    "    data_dir = os.path.join(BASE_DIR, \"raw_data\", str(year), f\"R{round_number}\")\n",
    "    os.makedirs(data_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    # Debug: Verify event schedule\n",
    "    schedule = fastf1.get_event_schedule(year)\n",
    "    print(f\"Debug - {year} Schedule Length: {len(schedule)}\")\n",
    "    event = schedule[schedule['RoundNumber'] == round_number]\n",
    "    if event.empty:\n",
    "        raise ValueError(f\"No event found for Round {round_number} in {year}. Available rounds: {schedule['RoundNumber'].tolist()}\")\n",
    "    print(f\"Debug - Selected Event: {event['EventName'].iloc[0]} (Round {round_number})\")\n",
    "\n",
    "    # Dictionary to store session data\n",
    "    session_data = {}\n",
    "\n",
    "    # Compute track metadata using FP1\n",
    "    try:\n",
    "        fp1_session = fastf1.get_session(year, event_name, 'FP1')\n",
    "        fp1_session.load(telemetry=True)\n",
    "        # Use accurate, full laps for track length\n",
    "        valid_laps = fp1_session.laps[fp1_session.laps['IsAccurate'] & fp1_session.laps['LapTime'].notna()]\n",
    "        top_laps = valid_laps.nlargest(5, 'LapTime')  # Top 5 accurate laps\n",
    "        lap_lengths = []\n",
    "        for _, lap in top_laps.iterrows():\n",
    "            telemetry = lap.get_telemetry()\n",
    "            lap_length = (telemetry['Distance'].max() - telemetry['Distance'].min()) / 1000  # km\n",
    "            lap_lengths.append(lap_length)\n",
    "        track_length = np.mean(lap_lengths)  # Average for consistency\n",
    "        target_info['track_length'] = track_length\n",
    "\n",
    "        # Average speed: based on fastest lap and computed track length\n",
    "        fastest_lap = fp1_session.laps.pick_fastest()\n",
    "        lap_time_seconds = fastest_lap['LapTime'].total_seconds()\n",
    "        avg_speed = (track_length / (lap_time_seconds / 3600))  # km/h\n",
    "        target_info['avg_speed'] = avg_speed\n",
    "\n",
    "        # Downforce level (heuristic)\n",
    "        telemetry = fastest_lap.get_telemetry()\n",
    "        speeds = telemetry['Speed']\n",
    "        top_speed = speeds.max()\n",
    "        corner_speeds = speeds[telemetry['Throttle'] < 90]  # Proxy for corners\n",
    "        avg_corner_speed = corner_speeds.mean() if not corner_speeds.empty else top_speed / 2\n",
    "        corner_to_straight_ratio = avg_corner_speed / top_speed\n",
    "        circuit_info = fp1_session.get_circuit_info()\n",
    "        corner_count = len(circuit_info.corners)\n",
    "        if corner_count > 15 and corner_to_straight_ratio > 0.7:\n",
    "            downforce_level = 'High'\n",
    "        elif corner_count > 10 and corner_to_straight_ratio > 0.5:\n",
    "            downforce_level = 'Medium-High'\n",
    "        elif corner_count > 5 and corner_to_straight_ratio > 0.3:\n",
    "            downforce_level = 'Medium'\n",
    "        else:\n",
    "            downforce_level = 'Low'\n",
    "        target_info['downforce_level'] = downforce_level\n",
    "\n",
    "        print(f\"\\nComputed Track Metadata (from FP1):\")\n",
    "        print(f\"Track Length: {track_length:.3f} km\")\n",
    "        print(f\"Average Speed: {avg_speed:.1f} km/h\")\n",
    "        print(f\"Downforce Level: {downforce_level}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to compute track metadata: {e}\")\n",
    "        print(f\"Warning: Track metadata computation failed: {e}\")\n",
    "        target_info['track_length'] = target_info.get('track_length', 'TBD')\n",
    "        target_info['avg_speed'] = target_info.get('avg_speed', 'TBD')\n",
    "        target_info['downforce_level'] = target_info.get('downforce_level', 'TBD')\n",
    "\n",
    "    # Download and process each session\n",
    "    for session_num, session_label in enumerate(session_labels, start=1):\n",
    "        print(f\"\\nDownloading Session {session_num} ({session_label}) data...\")\n",
    "        session = fastf1.get_session(year, event_name, session_label)\n",
    "        session.load(telemetry=True)\n",
    "        logger.info(f\"Loaded data for {session_label} with {len(session.laps)} laps.\")\n",
    "\n",
    "        # Extract lap data\n",
    "        laps = session.laps\n",
    "        session_data[session_label] = {'laps': laps}\n",
    "\n",
    "        # Save lap data\n",
    "        file_path = os.path.join(data_dir, f\"{year}_R{round_number}_{session_label.replace(' ', '_')}_laps.csv\")\n",
    "        laps.to_csv(file_path, index=False)\n",
    "        print(f\"Saved {session_label} lap data to {file_path}\")\n",
    "\n",
    "        # Extract weather data\n",
    "        weather = session.weather_data\n",
    "        session_data[session_label]['weather'] = weather\n",
    "        weather_file_path = os.path.join(data_dir, f\"{year}_R{round_number}_{session_label.replace(' ', '_')}_weather.csv\")\n",
    "        weather.to_csv(weather_file_path, index=False)\n",
    "        print(f\"Saved {session_label} weather data to {weather_file_path}\")\n",
    "\n",
    "        # Compute indicators\n",
    "        fastest_laps = laps.groupby('Driver')['LapTime'].min().reset_index()\n",
    "        avg_lap_times = laps.groupby('Driver')['LapTime'].mean().reset_index().rename(columns={'LapTime': 'AvgLapTime'})\n",
    "        tyre_degradation = laps[laps['Compound'].notna() & laps['TyreLife'].notna()]\n",
    "        tyre_degradation = tyre_degradation.groupby(['Driver', 'Stint']).agg({\n",
    "            'LapTime': lambda x: x.mean().total_seconds() if not x.empty else np.nan,\n",
    "            'TyreLife': 'mean'\n",
    "        }).reset_index()\n",
    "        # Refined degradation rate: tighter clip and fill NaN\n",
    "        tyre_degradation['DegradationRate'] = (\n",
    "            tyre_degradation.groupby('Driver')\n",
    "            .apply(lambda x: (x['LapTime'].diff() / x['TyreLife'].diff()).replace([np.inf, -np.inf], 0).clip(lower=-0.1, upper=0.1), include_groups=False)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        tyre_degradation = tyre_degradation.groupby('Driver')['DegradationRate'].mean().fillna(0).reset_index()\n",
    "        indicators = fastest_laps.merge(avg_lap_times, on='Driver').merge(tyre_degradation, on='Driver', how='left')\n",
    "        session_data[session_label]['indicators'] = indicators\n",
    "        indicators_file_path = os.path.join(data_dir, f\"{year}_R{round_number}_{session_label.replace(' ', '_')}_indicators.csv\")\n",
    "        indicators.to_csv(indicators_file_path, index=False)\n",
    "        print(f\"Saved {session_label} indicators to {indicators_file_path}\")\n",
    "\n",
    "        # Preview data\n",
    "        if os.getenv(\"SILENT\", \"false\").lower() != \"true\":\n",
    "            print(f\"\\n{session_label} Lap Data Preview (first 5 rows):\")\n",
    "            display(laps.head())\n",
    "            print(f\"\\n{session_label} Weather Data Preview (first 5 rows):\")\n",
    "            display(weather.head())\n",
    "            print(f\"\\n{session_label} Indicators (Fastest Lap, Avg Lap Time, Tyre Degradation):\")\n",
    "            display(indicators)\n",
    "\n",
    "    # Summary\n",
    "    if os.getenv(\"SILENT\", \"false\").lower() != \"true\":\n",
    "        print(\"\\nDownload Summary:\")\n",
    "        for session_label, data in session_data.items():\n",
    "            print(f\"{session_label}: {len(data['laps'])} laps, Columns: {list(data['laps'].columns)}\")\n",
    "            print(f\"  Weather: {len(data['weather'])} records\")\n",
    "            print(f\"  Indicators: {len(data['indicators'])} drivers\")\n",
    "\n",
    "    return session_data\n",
    "\n",
    "# Example usage\n",
    "session_data = download_practice_data(target_info)\n",
    "\n",
    "# Expected Inputs: target_info dict from Step 2 (year, round, session_types, event_name)\n",
    "# Expected Outputs: Track metadata, download confirmations, file paths, data previews, summary (unless SILENT=true)\n",
    "# Data Exported: CSV files for each session (laps, weather, indicators) in 'raw_data', updated target_info"
   ],
   "id": "5f613480fce24be7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 4 - Clean and Aggregate Practice Data\n",
    "# Objective: Clean lap data, aggregate driver stats across sessions, and prepare for predictions.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import display\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def clean_and_aggregate_data(target_info, session_data):\n",
    "    year = target_info['year']\n",
    "    round_number = target_info['round']\n",
    "    session_types = target_info['session_types']\n",
    "    BASE_DIR = os.getcwd()  # Jupyter-friendly\n",
    "    DATA_DIR = os.path.join(BASE_DIR, \"raw_data\", str(year), f\"R{round_number}\")\n",
    "    CLEANED_DIR = os.path.join(DATA_DIR, \"cleaned_data\")\n",
    "    os.makedirs(CLEANED_DIR, exist_ok=True)\n",
    "\n",
    "    if not session_data:\n",
    "        logger.error(\"No session data provided from Step 3.\")\n",
    "        return {}, pd.DataFrame()\n",
    "\n",
    "    cleaned_data = {}\n",
    "    driver_summary = []\n",
    "    total_raw_laps = 0\n",
    "    total_cleaned_laps = 0\n",
    "\n",
    "    for session_type, data in session_data.items():\n",
    "        logger.info(f\"Cleaning {session_type} data...\")\n",
    "        laps = data['laps']  # Use laps from session_data\n",
    "        if laps.empty:\n",
    "            logger.warning(f\"{session_type} data is empty.\")\n",
    "            continue\n",
    "        total_raw_laps += len(laps)\n",
    "        logger.info(f\"Raw Laps in {session_type}: {len(laps)}\")\n",
    "\n",
    "        # Clean laps: remove deleted, use accurate laps, drop incomplete\n",
    "        clean_laps = laps[(laps['Deleted'] == False) & (laps['IsAccurate'] == True)].copy()\n",
    "        logger.info(f\"After Deleting Removed/Keeping Accurate: {len(clean_laps)}\")\n",
    "        clean_laps = clean_laps.dropna(subset=['LapTime', 'Sector1Time', 'Sector2Time', 'Sector3Time'], how='all')\n",
    "        logger.info(f\"After Dropping Fully NaN Laps: {len(clean_laps)}\")\n",
    "\n",
    "        # Convert LapTime to seconds\n",
    "        clean_laps['LapTimeSeconds'] = pd.to_timedelta(clean_laps['LapTime'], errors='coerce').dt.total_seconds()\n",
    "        clean_laps = clean_laps[clean_laps['LapTimeSeconds'].notna() & (clean_laps['LapTimeSeconds'] > 0)]\n",
    "        total_cleaned_laps += len(clean_laps)\n",
    "\n",
    "        # Filter outliers and pit laps for average lap time\n",
    "        clean_for_avg = clean_laps[\n",
    "            (clean_laps['PitInTime'].isna()) &\n",
    "            (clean_laps['PitOutTime'].isna())\n",
    "        ]\n",
    "        driver_medians = clean_for_avg.groupby('Driver')['LapTimeSeconds'].median()\n",
    "        clean_for_avg = clean_for_avg.merge(driver_medians, on='Driver', suffixes=('', '_Median'))\n",
    "        clean_for_avg = clean_for_avg[clean_for_avg['LapTimeSeconds'] <= clean_for_avg['LapTimeSeconds_Median'] * 1.3].drop(columns=['LapTimeSeconds_Median'])\n",
    "        logger.info(f\"Laps for AvgLapTime (After Pit/Outlier Filter): {len(clean_for_avg)}\")\n",
    "\n",
    "        # Store cleaned data\n",
    "        cleaned_data[session_type] = clean_laps\n",
    "\n",
    "        # Driver stats per session\n",
    "        driver_stats = clean_for_avg.groupby('Driver').agg({\n",
    "            'LapTimeSeconds': ['min', 'mean', 'std', 'count']\n",
    "        }).reset_index()\n",
    "        driver_stats.columns = ['Driver', 'FastestLap', 'AvgLapTime', 'LapTimeStd', 'LapCount']\n",
    "        driver_stats['Session'] = session_type\n",
    "        driver_summary.append(driver_stats)\n",
    "\n",
    "        # Save cleaned laps\n",
    "        cleaned_file = os.path.join(CLEANED_DIR, f\"{year}_R{round_number}_{session_type.replace(' ', '_')}_cleaned_laps.csv\")\n",
    "        clean_laps.to_csv(cleaned_file, index=False)\n",
    "        logger.info(f\"Saved cleaned laps to {cleaned_file}\")\n",
    "\n",
    "    if not driver_summary:\n",
    "        logger.error(\"No driver data aggregated.\")\n",
    "        overall_summary = pd.DataFrame()\n",
    "    else:\n",
    "        all_driver_summary = pd.concat(driver_summary, ignore_index=True)\n",
    "        overall_summary = all_driver_summary.groupby('Driver').agg({\n",
    "            'FastestLap': 'min',\n",
    "            'AvgLapTime': 'mean',\n",
    "            'LapTimeStd': 'mean',\n",
    "            'LapCount': 'sum'\n",
    "        }).reset_index()\n",
    "        overall_summary = overall_summary.sort_values('FastestLap')\n",
    "        if os.getenv(\"SILENT\", \"false\").lower() != \"true\":\n",
    "            logger.info(f\"Total Raw Laps: {total_raw_laps}\")\n",
    "            logger.info(f\"Total Cleaned Laps: {total_cleaned_laps}\")\n",
    "            logger.info(\"Driver Summary Generated (Top 5):\")\n",
    "            print(overall_summary.head().to_string(index=False))\n",
    "            logger.info(\"Full Overall Driver Summary:\")\n",
    "            display(overall_summary)\n",
    "\n",
    "    output_file = os.path.join(CLEANED_DIR, f\"{year}_R{round_number}_practice_summary.csv\")\n",
    "    overall_summary.to_csv(output_file, index=False)\n",
    "    logger.info(f\"Saved aggregated practice summary to {output_file}\")\n",
    "\n",
    "    logger.info(f\"Cleaned data keys: {list(cleaned_data.keys())}\")\n",
    "    return cleaned_data, overall_summary\n",
    "\n",
    "# Example usage\n",
    "cleaned_data, driver_summary = clean_and_aggregate_data(target_info, session_data)\n",
    "\n",
    "# Why: Cleans raw lap data and aggregates driver performance stats for prediction.\n",
    "# How to Read: Shows cleaned lap counts, driver stats per session, and an overall summary sorted by fastest lap.\n",
    "# Expected Inputs: target_info (year, round, session_types), session_data from Step 3\n",
    "# Expected Outputs: Cleaned lap CSVs, practice summary CSV, previews of top 5 and full summary\n",
    "# Data Exported: Cleaned laps per session, overall driver summary"
   ],
   "id": "9f51413f20daa630",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 5: Unified Driver Performance Analysis\n",
    "# Objective: Compute fastest lap times, best sector times, base pace, variance, degradation slopes, and theoretical fastest lap, with visualizations and a dynamic conclusion.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def compute_driver_performance(cleaned_data, year, round_number, data_dir=\"raw_data\"):\n",
    "    \"\"\"\n",
    "    Compute fastest lap times, best sector times, base pace, variance, degradation slopes, and theoretical fastest lap.\n",
    "\n",
    "    Args:\n",
    "        cleaned_data (dict): Cleaned lap DataFrames from Step 4\n",
    "        year (int): Race year (e.g., 2024)\n",
    "        round_number (int): Round number (e.g., 3)\n",
    "        data_dir (str): Directory for output files (default: \"raw_data\")\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Driver performance metrics\n",
    "    \"\"\"\n",
    "    # Update data_dir to match the format raw_data/{year}/R{round_number}/\n",
    "    data_dir = os.path.join(data_dir, str(year), f\"R{round_number}\")\n",
    "\n",
    "    # Combine all practice sessions\n",
    "    all_laps = pd.concat([cleaned_data[session] for session in cleaned_data.keys()], ignore_index=True)\n",
    "\n",
    "    # Compute fastest lap time per driver\n",
    "    fastest_laps = all_laps.groupby('Driver').agg({\n",
    "        'LapTimeSeconds': 'min',\n",
    "        'Team': 'first'\n",
    "    }).reset_index()\n",
    "    fastest_laps = fastest_laps.rename(columns={'LapTimeSeconds': 'FastestLapTime'})\n",
    "\n",
    "    # Compute best sector times per driver\n",
    "    all_laps['Sector1Time'] = pd.to_timedelta(all_laps['Sector1Time'], errors='coerce').dt.total_seconds()\n",
    "    all_laps['Sector2Time'] = pd.to_timedelta(all_laps['Sector2Time'], errors='coerce').dt.total_seconds()\n",
    "    all_laps['Sector3Time'] = pd.to_timedelta(all_laps['Sector3Time'], errors='coerce').dt.total_seconds()\n",
    "    sector_performance = all_laps.groupby('Driver').agg({\n",
    "        'Sector1Time': 'min',\n",
    "        'Sector2Time': 'min',\n",
    "        'Sector3Time': 'min'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Compute theoretical fastest lap\n",
    "    sector_performance['TheoreticalLapTime'] = sector_performance['Sector1Time'] + sector_performance['Sector2Time'] + sector_performance['Sector3Time']\n",
    "\n",
    "    # Compute base pace and variance from long-run laps\n",
    "    long_run_laps = all_laps[(all_laps['TyreLife'] > 10) & (all_laps['IsAccurate'])]\n",
    "    if not long_run_laps.empty:\n",
    "        # Ensure sufficient long-run laps per driver\n",
    "        long_run_counts = long_run_laps.groupby('Driver').size()\n",
    "        insufficient_drivers = long_run_counts[long_run_counts < 3].index.tolist()\n",
    "        if insufficient_drivers:\n",
    "            print(f\"Warning: Insufficient long-run laps (<3) for drivers: {', '.join(insufficient_drivers)}. Using fallback for these drivers.\")\n",
    "\n",
    "        # Filter outliers using IQR\n",
    "        Q1 = long_run_laps['LapTimeSeconds'].quantile(0.25)\n",
    "        Q3 = long_run_laps['LapTimeSeconds'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        long_run_laps = long_run_laps[\n",
    "            (long_run_laps['LapTimeSeconds'] >= Q1 - 1.5 * IQR) &\n",
    "            (long_run_laps['LapTimeSeconds'] <= Q3 + 1.5 * IQR)\n",
    "        ]\n",
    "        base_pace = long_run_laps.groupby('Driver')['LapTimeSeconds'].mean().reset_index()\n",
    "        base_pace.columns = ['Driver', 'BasePace']\n",
    "        pace_variance = long_run_laps.groupby('Driver')['LapTimeSeconds'].std().reset_index()\n",
    "        pace_variance.columns = ['Driver', 'LapTimeVar']\n",
    "\n",
    "        # Compute degradation slope\n",
    "        deg_slope = long_run_laps.groupby(['Driver', 'Compound'], as_index=False).apply(\n",
    "            lambda x: pd.Series({'DegradationSlope': max(7.0, np.polyfit(x['TyreLife'], x['LapTimeSeconds'], 1)[0] * 100) if len(x) > 2 else 7.0}),\n",
    "            include_groups=False\n",
    "        ).reset_index(drop=True)\n",
    "        # Use the primary compound's degradation slope\n",
    "        deg_slope = deg_slope.groupby('Driver').agg({'DegradationSlope': 'mean'}).reset_index()\n",
    "    else:\n",
    "        base_pace = pd.DataFrame({'Driver': fastest_laps['Driver'], 'BasePace': fastest_laps['FastestLapTime'] + 5})  # Fallback\n",
    "        pace_variance = pd.DataFrame({'Driver': fastest_laps['Driver'], 'LapTimeVar': 0.01 * base_pace['BasePace']})\n",
    "        deg_slope = pd.DataFrame({'Driver': fastest_laps['Driver'], 'DegradationSlope': 7.0})\n",
    "\n",
    "    # Merge all metrics with outer joins to retain all drivers\n",
    "    driver_performance = fastest_laps.merge(sector_performance, on='Driver', how='left')\n",
    "    driver_performance = driver_performance.merge(base_pace, on='Driver', how='outer')\n",
    "    driver_performance = driver_performance.merge(pace_variance, on='Driver', how='outer')\n",
    "    driver_performance = driver_performance.merge(deg_slope, on='Driver', how='outer')\n",
    "\n",
    "    # Apply fallbacks for drivers with insufficient long-run data\n",
    "    mask = driver_performance['BasePace'].isna()\n",
    "    driver_performance.loc[mask, 'BasePace'] = driver_performance.loc[mask, 'FastestLapTime'] + 5\n",
    "    driver_performance.loc[mask, 'LapTimeVar'] = 0.01 * driver_performance.loc[mask, 'BasePace']\n",
    "    driver_performance.loc[mask, 'DegradationSlope'] = 7.0\n",
    "\n",
    "    # Ensure realistic minimums\n",
    "    driver_performance['DegradationSlope'] = driver_performance['DegradationSlope'].clip(lower=7.0)\n",
    "    driver_performance['LapTimeVar'] = driver_performance['LapTimeVar'].clip(lower=0.01 * driver_performance['BasePace'])\n",
    "\n",
    "    # Sort by TheoreticalLapTime\n",
    "    driver_performance = driver_performance.sort_values('TheoreticalLapTime')\n",
    "\n",
    "    # Display table with sector times and theoretical lap time\n",
    "    print(\"\\nDriver Performance (sorted by TheoreticalLapTime):\")\n",
    "    display(driver_performance[['Driver', 'FastestLapTime', 'TheoreticalLapTime', 'Sector1Time', 'Sector2Time', 'Sector3Time', 'BasePace', 'DegradationSlope', 'Team']])\n",
    "\n",
    "    # Visualization 1: Bar chart comparing FastestLapTime and TheoreticalLapTime\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bar_width = 0.35\n",
    "    index = range(len(driver_performance))\n",
    "    min_time = driver_performance['TheoreticalLapTime'].min() - 1\n",
    "    max_time = driver_performance['TheoreticalLapTime'].max() + 1\n",
    "    plt.bar([i - bar_width/2 for i in index], driver_performance['FastestLapTime'], bar_width, label='Fastest Lap Time', color='blue')\n",
    "    plt.bar([i + bar_width/2 for i in index], driver_performance['TheoreticalLapTime'], bar_width, label='Theoretical Lap Time', color='orange')\n",
    "    plt.xlabel('Driver')\n",
    "    plt.ylabel('Lap Time (seconds)')\n",
    "    plt.title(f'Fastest vs. Theoretical Lap Times ({year} Round {round_number})')\n",
    "    plt.xticks(index, driver_performance['Driver'], rotation=45, ha='right')\n",
    "    plt.ylim(min_time, max_time)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Visualization 2: Separate bar charts for each sector time\n",
    "    sectors = ['Sector1Time', 'Sector2Time', 'Sector3Time']\n",
    "    sector_titles = ['Sector 1 Time', 'Sector 2 Time', 'Sector 3 Time']\n",
    "    for sector, title in zip(sectors, sector_titles):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sorted_sector = driver_performance.sort_values(sector)\n",
    "        min_sector = sorted_sector[sector].min() - 0.5\n",
    "        max_sector = sorted_sector[sector].max() + 0.5\n",
    "        plt.bar(sorted_sector['Driver'], sorted_sector[sector], color='blue')\n",
    "        plt.xlabel('Driver')\n",
    "        plt.ylabel('Sector Time (seconds)')\n",
    "        plt.title(f'Best {title} per Driver ({year} Round {round_number})')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylim(min_sector, max_sector)\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Visualization 3: Bar chart for BasePace\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sorted_base_pace = driver_performance.sort_values('BasePace')\n",
    "    min_pace = sorted_base_pace['BasePace'].min() - 1\n",
    "    max_pace = sorted_base_pace['BasePace'].max() + 1\n",
    "    plt.bar(sorted_base_pace['Driver'], sorted_base_pace['BasePace'], color='green')\n",
    "    plt.xlabel('Driver')\n",
    "    plt.ylabel('Base Pace (seconds)')\n",
    "    plt.title(f'Base Pace per Driver ({year} Round {round_number})')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylim(min_pace, max_pace)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Visualization 4: Scatter plot of DegradationSlope vs. BasePace\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    min_slope = driver_performance['DegradationSlope'].min() - 2\n",
    "    max_slope = driver_performance['DegradationSlope'].max() + 2\n",
    "    min_pace_scatter = driver_performance['BasePace'].min() - 1\n",
    "    max_pace_scatter = driver_performance['BasePace'].max() + 1\n",
    "    plt.scatter(driver_performance['BasePace'], driver_performance['DegradationSlope'], c='red', label='Drivers')\n",
    "    for i, driver in enumerate(driver_performance['Driver']):\n",
    "        plt.text(driver_performance['BasePace'].iloc[i], driver_performance['DegradationSlope'].iloc[i], driver, fontsize=7, ha='right')\n",
    "    plt.xlabel('Base Pace (seconds)')\n",
    "    plt.ylabel('Degradation Slope (% per lap)')\n",
    "    plt.title(f'Degradation Slope vs. Base Pace ({year} Round {round_number})')\n",
    "    plt.ylim(min_slope, max_slope)\n",
    "    plt.xlim(min_pace_scatter, max_pace_scatter)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Visualization 5: Boxplot for BasePace per Driver\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    base_pace_boxes = [long_run_laps[long_run_laps['Driver'] == driver]['LapTimeSeconds'].dropna() for driver in driver_performance['Driver']]\n",
    "    plt.boxplot(base_pace_boxes, tick_labels=driver_performance['Driver'], patch_artist=True)\n",
    "    plt.ylabel('Base Pace (seconds)')\n",
    "    plt.title(f'Boxplot of Base Pace per Driver ({year} Round {round_number})')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Visualization 6: Bar chart for DegradationSlope per Driver (using calculated values)\n",
    "    print(\"\\nDebug: DegradationSlope values for plot:\", driver_performance['DegradationSlope'].tolist())\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sorted_deg_slope = driver_performance.sort_values('DegradationSlope', ascending=False)\n",
    "    plt.bar(sorted_deg_slope['Driver'], sorted_deg_slope['DegradationSlope'], color='purple')\n",
    "    plt.xlabel('Driver')\n",
    "    plt.ylabel('Degradation Slope (% per lap)')\n",
    "    plt.title(f'Degradation Slope per Driver ({year} Round {round_number})')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylim(6, 20)  # Set y-axis to match DegradationSlope range\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Dynamic Conclusion and Guide\n",
    "    print(\"\\n=== How to Read the Output ===\")\n",
    "    print(\"- **FastestLapTime**: The fastest actual lap time achieved by each driver across practice sessions, indicating peak performance potential for qualifying.\")\n",
    "    print(\"- **TheoreticalLapTime**: The sum of the best sector times (Sector1Time + Sector2Time + Sector3Time), representing the theoretical fastest lap if the driver perfectly combines their best sectors.\")\n",
    "    print(\"- **Sector1Time, Sector2Time, Sector3Time**: The best times for each sector, showing strengths in different parts of the track.\")\n",
    "    print(\"- **BasePace**: The average lap time on long runs (tyre life > 10 laps), reflecting race pace consistency.\")\n",
    "    print(\"- **DegradationSlope**: The rate of lap time increase per lap due to tyre degradation, higher values indicate faster tyre wear.\")\n",
    "    print(\"- **Boxplot of BasePace per Driver**: Shows the distribution of long-run lap times for each driver, with median, quartiles, and outliers indicating race pace consistency.\")\n",
    "    print(\"- **Bar Chart of DegradationSlope per Driver**: Displays the individual degradation slope values per driver, sorted from highest to lowest, highlighting tyre wear rates.\")\n",
    "\n",
    "    print(\"\\n=== Conclusion ===\")\n",
    "    print(\"This analysis provides insights into driver performance across practice sessions:\")\n",
    "    # Highlight fastest drivers\n",
    "    fastest_driver = driver_performance.loc[driver_performance['FastestLapTime'].idxmin(), 'Driver']\n",
    "    fastest_time = driver_performance['FastestLapTime'].min()\n",
    "    print(f\"- **Fastest Lap**: {fastest_driver} achieved the fastest lap time of {fastest_time:.2f} seconds, indicating strong qualifying potential.\")\n",
    "\n",
    "    # Highlight theoretical fastest lap\n",
    "    theoretical_fastest_driver = driver_performance.loc[driver_performance['TheoreticalLapTime'].idxmin(), 'Driver']\n",
    "    theoretical_fastest_time = driver_performance['TheoreticalLapTime'].min()\n",
    "    print(f\"- **Theoretical Fastest Lap**: {theoretical_fastest_driver} could theoretically achieve {theoretical_fastest_time:.2f} seconds by combining their best sector times.\")\n",
    "\n",
    "    # Highlight largest gap between fastest and theoretical lap\n",
    "    driver_performance['LapTimeGap'] = driver_performance['FastestLapTime'] - driver_performance['TheoreticalLapTime']\n",
    "    largest_gap_driver = driver_performance.loc[driver_performance['LapTimeGap'].idxmax(), 'Driver']\n",
    "    largest_gap = driver_performance['LapTimeGap'].max()\n",
    "    print(f\"- **Largest Gap**: {largest_gap_driver} has the largest gap between their fastest lap and theoretical lap ({largest_gap:.2f} seconds), indicating inconsistency across sectors.\")\n",
    "\n",
    "    # Highlight base pace anomalies dynamically\n",
    "    base_pace_median = driver_performance['BasePace'].median()\n",
    "    base_pace_q1 = driver_performance['BasePace'].quantile(0.25)\n",
    "    base_pace_q3 = driver_performance['BasePace'].quantile(0.75)\n",
    "    base_pace_outliers_low = driver_performance[driver_performance['BasePace'] < base_pace_q1 - 1.5 * (base_pace_q3 - base_pace_q1)]['Driver'].tolist()\n",
    "    if base_pace_outliers_low:\n",
    "        for driver in base_pace_outliers_low:\n",
    "            driver_base_pace = driver_performance[driver_performance['Driver'] == driver]['BasePace'].iloc[0]\n",
    "            print(f\"- **Base Pace Anomaly**: {driver}’s base pace ({driver_base_pace:.2f} seconds) is significantly lower than the median ({base_pace_median:.2f} seconds), possibly due to a setup optimized for short runs or insufficient long-run data. Verify with additional laps.\")\n",
    "\n",
    "    # Highlight race pace insights from boxplot\n",
    "    base_pace_outliers = driver_performance[(driver_performance['BasePace'] < base_pace_q1 - 1.5 * (base_pace_q3 - base_pace_q1)) |\n",
    "                                           (driver_performance['BasePace'] > base_pace_q3 + 1.5 * (base_pace_q3 - base_pace_q1))]['Driver'].tolist()\n",
    "    print(f\"- **Race Pace Insight**: The median base pace is {base_pace_median:.2f} seconds, with outliers including {', '.join(base_pace_outliers) if base_pace_outliers else 'none'}, indicating potential inconsistency or unique setups.\")\n",
    "\n",
    "    # Highlight tyre degradation insights dynamically\n",
    "    deg_slope_median = driver_performance['DegradationSlope'].median()\n",
    "    deg_slope_q1 = driver_performance['DegradationSlope'].quantile(0.25)\n",
    "    deg_slope_q3 = driver_performance['DegradationSlope'].quantile(0.75)\n",
    "    deg_slope_outliers = driver_performance[(driver_performance['DegradationSlope'] < deg_slope_q1 - 1.5 * (deg_slope_q3 - deg_slope_q1)) |\n",
    "                                           (driver_performance['DegradationSlope'] > deg_slope_q3 + 1.5 * (deg_slope_q3 - deg_slope_q1))]['Driver'].tolist()\n",
    "    print(f\"- **Tyre Degradation Insight**: The median degradation slope is {deg_slope_median:.2f}% per lap, with outliers including {', '.join(deg_slope_outliers) if deg_slope_outliers else 'none'}, suggesting potential tyre wear issues.\")\n",
    "\n",
    "    print(\"\\n**Next Steps**: Compare these metrics with telemetry data (Step 6) to assess car setup, sandbagging analysis (Step 7) for hidden pace, and track characteristics (Step 9) to understand track-specific advantages.\")\n",
    "\n",
    "    # Save to CSV\n",
    "    output_file = os.path.join(data_dir, f\"{year}_R{round_number}_driver_performance.csv\")\n",
    "    driver_performance.to_csv(output_file, index=False)\n",
    "    print(f\"Saved driver performance to {output_file}\")\n",
    "\n",
    "    return driver_performance\n",
    "\n",
    "# Example usage with dynamic target_info\n",
    "driver_performance_df = compute_driver_performance(cleaned_data, year=target_info['year'], round_number=target_info['round'])\n",
    "\n",
    "# Why: Analyzes driver performance metrics (fastest lap, sectors, base pace, degradation) for prediction.\n",
    "# How to Read: Displays a table of metrics, visualizations (bar charts, scatter, boxplot), and a dynamic conclusion.\n",
    "# Expected Inputs: cleaned_data from Step 4, year and round_number from target_info\n",
    "# Expected Outputs: Table, 6 visualizations, conclusion, CSV file\n",
    "# Data Exported: driver_performance.csv in raw_data/{year}/R{round_number}/ directory"
   ],
   "id": "eadd33a10d5af7e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 6 - Assess Telemetry Metrics\n",
    "# Objective: Analyze telemetry (top speed, throttle time, braking) to infer car setup and driver style, visualizing as bar charts.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import fastf1\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def analyze_telemetry_metrics(config):\n",
    "    \"\"\"\n",
    "    Analyze telemetry metrics (top speed, throttle time, braking) to infer car setup and driver style.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Configuration dictionary containing:\n",
    "            - 'year': The race year (e.g., 2024)\n",
    "            - 'round': The round number (e.g., 3)\n",
    "            - 'session_types': List of session labels (default: ['Practice 1', 'Practice 2', 'Practice 3'])\n",
    "            - 'data_dir': Directory for raw data (default: \"raw_data\")\n",
    "            - 'event_name': Event name (optional, for context)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with averaged telemetry metrics\n",
    "    \"\"\"\n",
    "    # Extract configuration\n",
    "    year = config.get('year')\n",
    "    round_number = config.get('round')  # Matches target_info key\n",
    "    session_labels = config.get('session_types', ['Practice 1', 'Practice 2', 'Practice 3'])\n",
    "    data_dir = os.path.join(\"raw_data\", str(year), f\"R{round_number}\")  # Updated to match project structure\n",
    "    event_name = config.get('event_name', \"Unknown Event\")\n",
    "\n",
    "    # Set base directory dynamically\n",
    "    BASE_DIR = os.getcwd()  # Jupyter-friendly, avoids __file__ issues in notebooks\n",
    "\n",
    "    # Enable caching and set cache directory\n",
    "    cache_dir = os.path.join(BASE_DIR, \"cache\")\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "    fastf1.Cache.enable_cache(cache_dir)\n",
    "\n",
    "    # Load raw session data for telemetry\n",
    "    telemetry_metrics = []\n",
    "    for session_label in session_labels:\n",
    "        logger.info(f\"Loading {session_label} from cache or downloading for telemetry...\")\n",
    "        try:\n",
    "            session = fastf1.get_session(year, round_number, session_label)\n",
    "            session.load(laps=True, telemetry=True, weather=True)  # Load full session data without force_load\n",
    "            laps = session.laps\n",
    "\n",
    "            # Extract telemetry for a sample lap per driver (e.g., fastest lap)\n",
    "            driver_metrics = {}\n",
    "            for driver in laps['Driver'].unique():\n",
    "                driver_laps = laps[laps['Driver'] == driver]\n",
    "                fastest_lap = driver_laps.loc[driver_laps['LapTime'].idxmin()]\n",
    "                if not pd.isna(fastest_lap['LapTime']):\n",
    "                    telemetry = fastest_lap.get_telemetry()\n",
    "                    # Metrics: Max Speed, % Time at Full Throttle, Avg Braking Intensity\n",
    "                    max_speed = telemetry['Speed'].max()\n",
    "                    throttle_time = (telemetry['Throttle'] == 100).mean() * 100  # % time at full throttle\n",
    "                    braking_intensity = telemetry['Brake'].mean()  # Avg brake application (0-1)\n",
    "                    driver_metrics[driver] = {\n",
    "                        'Driver': driver,\n",
    "                        'MaxSpeed': max_speed,\n",
    "                        'ThrottleTime': throttle_time,\n",
    "                        'BrakingIntensity': braking_intensity,\n",
    "                        'Session': session_label\n",
    "                    }\n",
    "\n",
    "            # Convert to DataFrame and append\n",
    "            session_df = pd.DataFrame.from_dict(driver_metrics, orient='index')\n",
    "            telemetry_metrics.append(session_df)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error loading {session_label}: {e}. Skipping session.\")\n",
    "            continue\n",
    "\n",
    "    if not telemetry_metrics:\n",
    "        logger.error(\"No telemetry data available to process.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Combine and average metrics across sessions\n",
    "    all_telemetry = pd.concat(telemetry_metrics, ignore_index=True)\n",
    "    overall_telemetry = all_telemetry.groupby('Driver').mean(numeric_only=True).reset_index()\n",
    "\n",
    "    # Merge with team info (from FP3 raw data)\n",
    "    fp3_file = os.path.join(BASE_DIR, data_dir, f\"{year}_R{round_number}_Practice_3_laps.csv\")  # Updated to match Step 3 naming\n",
    "    if os.path.exists(fp3_file):\n",
    "        fp3_data = pd.read_csv(fp3_file)\n",
    "        team_info = fp3_data[['Driver', 'Team']].drop_duplicates()\n",
    "        overall_telemetry = overall_telemetry.merge(team_info, on='Driver', how='left')\n",
    "    else:\n",
    "        logger.warning(f\"Warning: {fp3_file} not found. Team info will be incomplete.\")\n",
    "        overall_telemetry['Team'] = 'Unknown'  # Default value if no team data\n",
    "\n",
    "    # Display table\n",
    "    print(\"\\nTelemetry Metrics (Top 10 by Max Speed):\")\n",
    "    display(overall_telemetry.sort_values('MaxSpeed', ascending=False).head(10))\n",
    "\n",
    "    # Visualization with bar charts\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    metrics = ['MaxSpeed', 'ThrottleTime', 'BrakingIntensity']\n",
    "    titles = ['Max Speed (km/h)', '% Time at Full Throttle', 'Avg Braking Intensity (0-1)']\n",
    "    colors = ['blue', 'orange', 'green']\n",
    "\n",
    "    for i, (metric, title, color) in enumerate(zip(metrics, titles, colors), 1):\n",
    "        plt.subplot(3, 1, i)\n",
    "        # For ThrottleTime, filter out drivers with 0% (invalid data)\n",
    "        if metric == 'ThrottleTime':\n",
    "            plot_data = overall_telemetry[overall_telemetry['ThrottleTime'] > 0]\n",
    "        else:\n",
    "            plot_data = overall_telemetry\n",
    "        sorted_data = plot_data.sort_values(metric, ascending=False)\n",
    "        plt.bar(sorted_data['Driver'], sorted_data[metric], color=color)\n",
    "\n",
    "        # Adjust y-axis limits for better readability\n",
    "        y_min = sorted_data[metric].min()\n",
    "        y_max = sorted_data[metric].max()\n",
    "        y_range = y_max - y_min\n",
    "        plt.ylim(y_min - 0.05 * y_range, y_max + 0.05 * y_range)\n",
    "\n",
    "        plt.xlabel('Driver')\n",
    "        plt.ylabel(title)\n",
    "        plt.title(f'Telemetry Metrics - {metric} ({year} {event_name})')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluation and Conclusion\n",
    "    print(\"\\n=== Evaluation and Conclusion ===\")\n",
    "    print(f\"Telemetry metrics for {year} Round {round_number} - {event_name} provide insights into car setup and driver behavior:\")\n",
    "\n",
    "    # Identify top performers and outliers\n",
    "    top_speed_driver = overall_telemetry.loc[overall_telemetry['MaxSpeed'].idxmax(), 'Driver']\n",
    "    top_speed_value = overall_telemetry['MaxSpeed'].max()\n",
    "    low_speed_driver = overall_telemetry.loc[overall_telemetry['MaxSpeed'].idxmin(), 'Driver']\n",
    "    low_speed_value = overall_telemetry['MaxSpeed'].min()\n",
    "\n",
    "    # Filter out invalid ThrottleTime for evaluation\n",
    "    valid_throttle = overall_telemetry[overall_telemetry['ThrottleTime'] > 0]\n",
    "    if not valid_throttle.empty:\n",
    "        top_throttle_driver = valid_throttle.loc[valid_throttle['ThrottleTime'].idxmax(), 'Driver']\n",
    "        top_throttle_value = valid_throttle['ThrottleTime'].max()\n",
    "        low_throttle_driver = valid_throttle.loc[valid_throttle['ThrottleTime'].idxmin(), 'Driver']\n",
    "        low_throttle_value = valid_throttle['ThrottleTime'].min()\n",
    "    else:\n",
    "        top_throttle_driver = \"N/A\"\n",
    "        top_throttle_value = \"N/A\"\n",
    "        low_throttle_driver = \"N/A\"\n",
    "        low_throttle_value = \"N/A\"\n",
    "\n",
    "    top_braking_driver = overall_telemetry.loc[overall_telemetry['BrakingIntensity'].idxmax(), 'Driver']\n",
    "    top_braking_value = overall_telemetry['BrakingIntensity'].max()\n",
    "    low_braking_driver = overall_telemetry.loc[overall_telemetry['BrakingIntensity'].idxmin(), 'Driver']\n",
    "    low_braking_value = overall_telemetry['BrakingIntensity'].min()\n",
    "\n",
    "    # Generic insights with dynamic data\n",
    "    print(\"- **Max Speed**: Indicates straight-line performance. Higher values suggest setups optimized for speed, which can be advantageous on tracks with long straights or high-speed sections.\")\n",
    "    print(f\"  - Top Performer: {top_speed_driver} with {top_speed_value:.2f} km/h, indicating strong straight-line speed.\")\n",
    "    print(f\"  - Lowest: {low_speed_driver} with {low_speed_value:.2f} km/h, possibly prioritizing downforce for cornering over top speed.\")\n",
    "\n",
    "    print(\"- **Throttle Time**: Reflects driving style or setup focus. High percentages indicate aggressive driving or setups designed for sustained high-speed performance, while low values may suggest a focus on technical corners or conservative driving.\")\n",
    "    if not valid_throttle.empty:\n",
    "        print(f\"  - Top Performer: {top_throttle_driver} with {top_throttle_value:.2f}% time at full throttle, suggesting an aggressive approach or high-speed setup.\")\n",
    "        print(f\"  - Lowest: {low_throttle_driver} with {low_throttle_value:.2f}% time at full throttle, which may indicate a setup favoring cornering or potential sandbagging.\")\n",
    "    else:\n",
    "        print(\"  - Note: Throttle time data is missing or invalid for all drivers, which may indicate telemetry issues. Investigate data collection for accuracy.\")\n",
    "\n",
    "    # Note missing throttle time drivers\n",
    "    missing_throttle = overall_telemetry[overall_telemetry['ThrottleTime'] == 0]['Driver'].tolist()\n",
    "    if missing_throttle:\n",
    "        print(f\"  - Missing Throttle Time: The following drivers have 0% throttle time, which may indicate telemetry errors: {', '.join(missing_throttle)}. These drivers are excluded from the throttle time chart.\")\n",
    "\n",
    "    print(\"- **Braking Intensity**: Measures average brake application (0-1). Higher values indicate heavier braking, often associated with late-braking techniques, while lower values suggest smoother driving, which can be advantageous in technical sections.\")\n",
    "    print(f\"  - Top Performer: {top_braking_driver} with an intensity of {top_braking_value:.3f}, potentially excelling in late braking into corners.\")\n",
    "    print(f\"  - Lowest: {low_braking_driver} with an intensity of {low_braking_value:.3f}, indicating a smoother braking style that may preserve tyre life.\")\n",
    "\n",
    "    # Combined interpretation\n",
    "    print(\"\\n**Combined Interpretation**:\")\n",
    "    print(\"Drivers with high max speed and throttle time are likely to perform well in qualifying, especially on tracks favoring straight-line speed. Those with balanced braking intensity may excel in races, particularly on circuits with technical corners where tyre management and cornering stability are key.\")\n",
    "    print(\"Look for outliers: Drivers with unusually low throttle time or high braking intensity may be sandbagging or using setups that prioritize race pace over qualifying performance. Correlate these metrics with lap times (Step 5) and sandbagging analysis (Step 7) to identify potential hidden pace.\")\n",
    "\n",
    "    # Save table\n",
    "    output_file = os.path.join(BASE_DIR, data_dir, f\"{year}_R{round_number}_telemetry_metrics.csv\")\n",
    "    overall_telemetry.to_csv(output_file, index=False)\n",
    "    print(f\"Saved telemetry metrics to {output_file}\")\n",
    "\n",
    "    return overall_telemetry\n",
    "\n",
    "# Example usage with dynamic target_info\n",
    "telemetry_df = analyze_telemetry_metrics(target_info)\n",
    "\n",
    "# Why: Telemetry reveals car setup (speed, downforce) and driver aggression, key for pole and race.\n",
    "# How to Read: Higher Max Speed = straight-line strength. High Throttle = aggressive driving. Moderate Braking = technical skill.\n",
    "# Expected Inputs: target_info with year, round, session_types, event_name\n",
    "# Expected Outputs: Table of metrics, bar charts, evaluation, saved CSV\n",
    "# Data Exported: {year}_R{round}_telemetry_metrics.csv in raw_data/{year}/R{round_number}/ directory"
   ],
   "id": "f49ea5cc2fd5efb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 7: Sandbagging Analysis (Updated to Use Driver Performance Data)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "if not logger.handlers:\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "def investigate_sandbagging(config):\n",
    "    \"\"\"\n",
    "    Detect teams hiding pace by comparing lap times to telemetry metrics.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Configuration dictionary containing:\n",
    "            - 'year': The race year (e.g., 2024)\n",
    "            - 'round': The round number (e.g., 3)\n",
    "            - 'data_dir': Directory for raw data (default: \"raw_data\")\n",
    "            - 'sandbag_threshold': Discrepancy threshold for sandbagging (default: 0.15 seconds)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Sandbagging analysis results\n",
    "    \"\"\"\n",
    "    year = config.get('year')\n",
    "    round_number = config.get('round')  # Matches target_info\n",
    "    data_dir = os.path.join(\"raw_data\", str(year), f\"R{round_number}\")  # Updated to match project structure\n",
    "    sandbag_threshold = config.get('sandbag_threshold', 0.15)\n",
    "\n",
    "    BASE_DIR = os.getcwd()  # Jupyter-friendly, avoids __file__ issues\n",
    "    logger.info(f\"Base directory: {BASE_DIR}\")\n",
    "\n",
    "    # Load data\n",
    "    driver_performance_file = os.path.join(BASE_DIR, data_dir, f\"{year}_R{round_number}_driver_performance.csv\")\n",
    "    telemetry_file = os.path.join(BASE_DIR, data_dir, f\"{year}_R{round_number}_telemetry_metrics.csv\")\n",
    "    if not os.path.exists(driver_performance_file):\n",
    "        logger.error(f\"Driver performance file not found: {driver_performance_file}\")\n",
    "        return pd.DataFrame()\n",
    "    if not os.path.exists(telemetry_file):\n",
    "        logger.error(f\"Telemetry metrics file not found: {telemetry_file}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    driver_performance = pd.read_csv(driver_performance_file)\n",
    "    telemetry_metrics = pd.read_csv(telemetry_file)\n",
    "\n",
    "    # Merge data\n",
    "    sandbag_analysis = driver_performance[['Driver', 'FastestLapTime', 'Team']].merge(\n",
    "        telemetry_metrics[['Driver', 'MaxSpeed', 'ThrottleTime']], on='Driver', how='left'\n",
    "    )\n",
    "    # Check for NaN values in telemetry metrics\n",
    "    if sandbag_analysis[['MaxSpeed', 'ThrottleTime']].isna().any().any():\n",
    "        logger.warning(\"NaN values found in telemetry metrics. Filling with median values.\")\n",
    "        sandbag_analysis['MaxSpeed'] = sandbag_analysis['MaxSpeed'].fillna(telemetry_metrics['MaxSpeed'].median())\n",
    "        sandbag_analysis['ThrottleTime'] = sandbag_analysis['ThrottleTime'].fillna(telemetry_metrics['ThrottleTime'].median())\n",
    "\n",
    "    sandbag_analysis['ExpectedLapTime'] = driver_performance['Sector1Time'] + driver_performance['Sector2Time'] + driver_performance['Sector3Time']\n",
    "    sandbag_analysis['LapTimeDiscrepancy'] = sandbag_analysis['FastestLapTime'] - sandbag_analysis['ExpectedLapTime']\n",
    "\n",
    "    # Predict FastestLapTime from MaxSpeed using linear regression\n",
    "    X = sandbag_analysis[['MaxSpeed']].values\n",
    "    y = sandbag_analysis['FastestLapTime'].values\n",
    "    reg = LinearRegression().fit(X, y)\n",
    "    sandbag_analysis['PredictedLapTime'] = reg.predict(X)\n",
    "    sandbag_analysis['PredictionDiscrepancy'] = sandbag_analysis['FastestLapTime'] - sandbag_analysis['PredictedLapTime']\n",
    "    sandbag_analysis['SpeedRank'] = sandbag_analysis['MaxSpeed'].rank(ascending=False)\n",
    "    sandbag_analysis['LapTimeRank'] = sandbag_analysis['FastestLapTime'].rank()\n",
    "\n",
    "    # New sandbagging logic: Flag drivers with high MaxSpeed (above mean) and FastestLapTime exceeding PredictedLapTime + threshold\n",
    "    mean_max_speed = sandbag_analysis['MaxSpeed'].mean()\n",
    "    sandbag_analysis['SandbagFlag'] = (sandbag_analysis['MaxSpeed'] > mean_max_speed) & (sandbag_analysis['PredictionDiscrepancy'] > sandbag_threshold)\n",
    "\n",
    "    # Display potential sandbaggers\n",
    "    print(\"\\nPotential Sandbaggers (Sorted by PredictionDiscrepancy):\")\n",
    "    display(sandbag_analysis[sandbag_analysis['SandbagFlag']].sort_values('PredictionDiscrepancy', ascending=False))\n",
    "\n",
    "    # Visualization: Scatter plot of FastestLapTime vs. MaxSpeed with sandbaggers highlighted\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    colors = ['blue' if not flag else 'red' for flag in sandbag_analysis['SandbagFlag']]\n",
    "    scatter = plt.scatter(sandbag_analysis['FastestLapTime'], sandbag_analysis['MaxSpeed'], c=colors, label='Drivers', alpha=0.6)\n",
    "    plt.axhline(y=mean_max_speed, color='gray', linestyle='--', label='Mean Max Speed')\n",
    "    plt.axvline(x=sandbag_analysis['PredictedLapTime'].mean() + sandbag_threshold, color='green', linestyle='--', label=f'Threshold (Predicted + {sandbag_threshold}s)')\n",
    "    # Plot regression line\n",
    "    x_range = np.array([[sandbag_analysis['MaxSpeed'].min()], [sandbag_analysis['MaxSpeed'].max()]])\n",
    "    y_pred = reg.predict(x_range)\n",
    "    plt.plot(y_pred, x_range, color='orange', linestyle='-', label='Regression Line')\n",
    "    for i, row in sandbag_analysis.iterrows():\n",
    "        plt.text(row['FastestLapTime'], row['MaxSpeed'], row['Driver'], fontsize=8, ha='right', va='bottom')\n",
    "        if row['SandbagFlag']:\n",
    "            plt.annotate(row['Driver'], (row['FastestLapTime'], row['MaxSpeed']), xytext=(5, 5), textcoords='offset points', fontsize=10, color='red', weight='bold')\n",
    "    plt.xlabel('Fastest Lap Time (seconds)')\n",
    "    plt.ylabel('Max Speed (km/h)')\n",
    "    plt.title(f'Sandbagging Analysis: Fastest Lap Time vs. Max Speed ({year} Round {round_number})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Save results\n",
    "    output_file = os.path.join(BASE_DIR, data_dir, f\"{year}_R{round_number}_sandbag_analysis.csv\")\n",
    "    sandbag_analysis.to_csv(output_file, index=False)\n",
    "    print(f\"Saved sandbag analysis to {output_file}\")\n",
    "\n",
    "    return sandbag_analysis\n",
    "\n",
    "# Example usage with dynamic target_info\n",
    "sandbag_df = investigate_sandbagging(target_info)\n",
    "\n",
    "# Why: Detects teams hiding pace by comparing lap times to telemetry, aiding race prediction.\n",
    "# How to Read: High PredictionDiscrepancy with high MaxSpeed flags sandbagging. Red dots on scatter plot highlight suspects.\n",
    "# Expected Inputs: driver_performance.csv, telemetry_metrics.csv from prior steps\n",
    "# Expected Outputs: Table of sandbaggers, scatter plot, saved CSV\n",
    "# Data Exported: {year}_R{round_number}_sandbag_analysis.csv in raw_data/{year}/R{round_number}/ directory"
   ],
   "id": "67ea2872616057de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 8 - Evaluate Track Evolution and Weather Impact\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import fastf1\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def evaluate_track_evolution(target_info, cleaned_data):\n",
    "    year = target_info['year']\n",
    "    round_number = target_info['round']\n",
    "    session_labels = target_info['session_types']\n",
    "    weather_metric = target_info.get('weather_metric', 'TrackTemp')\n",
    "    session_label_map = {\n",
    "        'FP1': 'Practice 1', 'FP2': 'Practice 2', 'FP3': 'Practice 3',\n",
    "        'Sprint Qualifying': 'Sprint Qualifying', 'Sprint': 'Sprint'\n",
    "    }\n",
    "    session_labels = [session_label_map.get(label, label) for label in session_labels]\n",
    "    event_name = target_info.get('event_name', 'Unknown Event')\n",
    "    data_dir = os.path.join(os.getcwd(), \"raw_data\", str(year), f\"R{round_number}\")\n",
    "\n",
    "    # Lap stats from cleaned_data\n",
    "    lap_stats = {}\n",
    "    for session_label in session_labels:\n",
    "        df = cleaned_data.get(session_label, pd.DataFrame())\n",
    "        if df.empty:\n",
    "            print(f\"Warning: No lap data for {session_label}, skipping.\")\n",
    "            continue\n",
    "        if pd.api.types.is_string_dtype(df['LapTime']):\n",
    "            df['LapTime'] = pd.to_timedelta(df['LapTime'])\n",
    "        lap_times = df['LapTime'].dt.total_seconds().dropna()\n",
    "        if len(lap_times) == 0:\n",
    "            print(f\"Warning: No valid lap times for {session_label}, skipping.\")\n",
    "            continue\n",
    "        lap_stats[session_label] = {\n",
    "            'mean': lap_times.mean(),\n",
    "            'std': lap_times.std(),\n",
    "            'count': len(lap_times)\n",
    "        }\n",
    "        print(f\"Average lap time for {session_label}: {lap_stats[session_label]['mean']:.3f} seconds (±{lap_stats[session_label]['std']:.3f})\")\n",
    "\n",
    "    # Weather data\n",
    "    weather_data = {}\n",
    "    for session_label in session_labels:\n",
    "        print(f\"Loading weather for {session_label} from FastF1...\")\n",
    "        try:\n",
    "            session = fastf1.get_session(year, round_number, session_label)\n",
    "            session.load(weather=True, laps=False)\n",
    "            weather = session.weather_data\n",
    "            print(f\"Available weather columns for {session_label}: {weather.columns.tolist()}\")\n",
    "            available_columns = [col for col in ['AirTemp', 'TrackTemp', 'Humidity', 'Rainfall'] if col in weather.columns]\n",
    "            if available_columns:\n",
    "                weather_data[session_label] = weather[available_columns]\n",
    "            else:\n",
    "                print(f\"No valid weather data for {session_label}, skipping weather analysis.\")\n",
    "                weather_data[session_label] = pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {session_label} weather data: {e}\")\n",
    "            weather_data[session_label] = pd.DataFrame()\n",
    "\n",
    "    avg_weather = {session: weather.mean() for session, weather in weather_data.items() if not weather.empty}\n",
    "\n",
    "    # Tables\n",
    "    lap_time_df = pd.DataFrame([\n",
    "        {'Session': session, 'AvgLapTime': stats['mean'], 'StdLapTime': stats['std'], 'LapCount': stats['count']}\n",
    "        for session, stats in lap_stats.items()\n",
    "    ])\n",
    "    print(\"\\nAverage Lap Times Across Sessions:\")\n",
    "    display(lap_time_df)\n",
    "\n",
    "    weather_list = [\n",
    "        {'Session': session, 'WeatherType': col, 'Value': value}\n",
    "        for session, stats in avg_weather.items() if not stats.empty\n",
    "        for col, value in stats.to_dict().items()\n",
    "    ]\n",
    "    weather_df = pd.DataFrame(weather_list) if weather_list else pd.DataFrame()\n",
    "    if not weather_df.empty:\n",
    "        print(\"\\nAverage Weather Conditions Across Sessions:\")\n",
    "        display(weather_df.pivot(index='Session', columns='WeatherType', values='Value'))\n",
    "\n",
    "    # Correlation\n",
    "    if not weather_df.empty and weather_metric in weather_df.pivot(index='Session', columns='WeatherType', values='Value').columns:\n",
    "        weather_values = weather_df[weather_df['WeatherType'] == weather_metric].set_index('Session')['Value']\n",
    "        lap_times = lap_time_df.set_index('Session')['AvgLapTime']\n",
    "        common_sessions = weather_values.index.intersection(lap_times.index)\n",
    "        if len(common_sessions) > 1:\n",
    "            weather_values = weather_values[common_sessions]\n",
    "            lap_times = lap_times[common_sessions]\n",
    "            correlation, _ = pearsonr(weather_values, lap_times)\n",
    "            print(f\"\\nCorrelation between {weather_metric} and AvgLapTime: {correlation:.3f}\")\n",
    "        else:\n",
    "            print(f\"\\nNot enough overlapping sessions for correlation analysis with {weather_metric}.\")\n",
    "    else:\n",
    "        print(f\"\\n{weather_metric} data not available for correlation analysis.\")\n",
    "\n",
    "    # Plot\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    ax1.plot(lap_time_df['Session'], lap_time_df['AvgLapTime'], marker='o', color='blue', label='Avg Lap Time (s)')\n",
    "    ax1.fill_between(lap_time_df['Session'], lap_time_df['AvgLapTime'] - lap_time_df['StdLapTime'] / 2,\n",
    "                     lap_time_df['AvgLapTime'] + lap_time_df['StdLapTime'] / 2, color='blue', alpha=0.2, label='Lap Time Std Dev (±0.5σ)')\n",
    "    ax1.set_xlabel('Session')\n",
    "    ax1.set_ylabel('Average Lap Time (seconds)', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    ax1.set_xticks(lap_time_df['Session'])\n",
    "    ax1.set_xticklabels(['P1', 'P2', 'P3'], rotation=45)\n",
    "\n",
    "    if not weather_df.empty and weather_metric in weather_df.pivot(index='Session', columns='WeatherType', values='Value').columns:\n",
    "        ax2 = ax1.twinx()\n",
    "        weather_values_df = weather_df[weather_df['WeatherType'] == weather_metric].set_index('Session')['Value']\n",
    "        weather_values_df = weather_values_df.reindex(lap_time_df['Session'])\n",
    "        ax2.plot(lap_time_df['Session'], weather_values_df, marker='o', color='orange', label=f'{weather_metric} (°C)')\n",
    "        ax2.set_ylabel(f'{weather_metric} (°C)', color='orange')\n",
    "        ax2.tick_params(axis='y', labelcolor='orange')\n",
    "        ax2.set_ylim(weather_values_df.min() - 2, weather_values_df.max() + 2)\n",
    "\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    if 'ax2' in locals():\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax1.legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "    else:\n",
    "        ax1.legend(loc='upper left')\n",
    "\n",
    "    plt.title(f'Track Evolution - {year} {event_name} (Round {round_number})')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Save\n",
    "    output_file = os.path.join(data_dir, f\"{year}_R{round_number}_track_evolution.csv\")\n",
    "    lap_time_df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved track evolution to {output_file}\")\n",
    "    return lap_time_df\n",
    "\n",
    "# Standalone execution for debugging (runs only if cell is executed directly)\n",
    "if 'target_info' in globals() and 'cleaned_data' in globals():\n",
    "    track_evolution_df = evaluate_track_evolution(target_info, cleaned_data)\n",
    "else:\n",
    "    print(\"Warning: target_info and/or cleaned_data not found. Run Cells 2 and 4 first for standalone execution.\")"
   ],
   "id": "efa03237c9fc0668",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 9 - Analyze Track Characteristics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import fastf1\n",
    "import os\n",
    "from scipy.stats import zscore\n",
    "\n",
    "def analyze_track_characteristics(target_info, cleaned_data):\n",
    "    year = target_info['year']\n",
    "    round_number = target_info['round']\n",
    "    event_name = target_info.get('event_name', 'Unknown Event')\n",
    "    data_dir = os.path.join(os.getcwd(), \"raw_data\", str(year), f\"R{round_number}\")\n",
    "\n",
    "    all_laps = pd.concat([cleaned_data[session] for session in cleaned_data.keys()], ignore_index=True)\n",
    "    required_columns = ['Driver', 'Team', 'SpeedST', 'Sector1Time', 'Sector2Time', 'Sector3Time', 'LapTime']\n",
    "    if not all(col in all_laps.columns for col in required_columns):\n",
    "        missing_cols = [col for col in required_columns if col not in all_laps.columns]\n",
    "        print(f\"Warning: Missing columns in cleaned_data: {missing_cols}. Using available data.\")\n",
    "        all_laps = all_laps.dropna(subset=[col for col in required_columns if col in all_laps.columns])\n",
    "\n",
    "    track_analysis = all_laps.groupby(['Driver', 'Team']).agg({\n",
    "        'SpeedST': 'max',\n",
    "        'Sector1Time': 'mean',\n",
    "        'Sector2Time': 'mean',\n",
    "        'Sector3Time': 'mean',\n",
    "        'LapTime': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    for col in ['Sector1Time', 'Sector2Time', 'Sector3Time', 'LapTime']:\n",
    "        if pd.api.types.is_string_dtype(track_analysis[col]):\n",
    "            track_analysis[col] = pd.to_timedelta(track_analysis[col])\n",
    "        track_analysis[col] = track_analysis[col].dt.total_seconds()\n",
    "\n",
    "    try:\n",
    "        session = fastf1.get_session(year, round_number, 'Practice 3')\n",
    "        session.load(laps=True, telemetry=True, weather=False)\n",
    "        circuit_info = session.get_circuit_info()\n",
    "        lap_length = circuit_info.length / 1000 if hasattr(circuit_info, 'length') else target_info.get('track_length', 6.174)\n",
    "        drs_zones = len(circuit_info.drs_zones) if hasattr(circuit_info, 'drs_zones') and circuit_info.drs_zones else 3\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching circuit info: {e}\")\n",
    "        lap_length = target_info.get('track_length', 6.174)\n",
    "        drs_zones = 3\n",
    "\n",
    "    avg_sector_speed = all_laps['SpeedST'].mean()\n",
    "    max_sector_speed = all_laps['SpeedST'].max()\n",
    "    downforce_requirement = 'Low' if avg_sector_speed > 330 else 'Medium-High' if avg_sector_speed > 310 else 'High'\n",
    "    overtaking_difficulty = 'Easy' if drs_zones > 2 else 'Moderate' if drs_zones == 2 else 'Hard'\n",
    "    try:\n",
    "        driver_performance = pd.read_csv(os.path.join(data_dir, f\"{year}_R{round_number}_driver_performance.csv\"))\n",
    "        avg_tyre_degradation = driver_performance['DegradationSlope'].mean()\n",
    "        tyre_degradation = 'High' if avg_tyre_degradation > 10 else 'Moderate' if avg_tyre_degradation > 7 else 'Low'\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading driver performance data: {e}\")\n",
    "        tyre_degradation = 'Moderate'\n",
    "    suits_fast_cars = 'Yes' if max_sector_speed > 330 else 'No'\n",
    "\n",
    "    track_characteristics = {\n",
    "        'Track': event_name,\n",
    "        'DownforceRequirement': downforce_requirement,\n",
    "        'OvertakingDifficulty': overtaking_difficulty,\n",
    "        'DRSZones': drs_zones,\n",
    "        'TyreDegradation': tyre_degradation,\n",
    "        'LapLength': lap_length,\n",
    "        'SuitsFastCars': suits_fast_cars\n",
    "    }\n",
    "\n",
    "    track_analysis['StraightSpeedPotential'] = track_analysis['SpeedST']\n",
    "    track_analysis['CorneringPotential'] = (track_analysis['Sector1Time'] + track_analysis['Sector2Time'] + track_analysis['Sector3Time']) / 3\n",
    "    track_analysis['AvgSpeed'] = (lap_length * 3600) / track_analysis['LapTime']\n",
    "    straight_line_ratio = track_analysis['StraightSpeedPotential'].mean() / track_analysis['CorneringPotential'].mean()\n",
    "    weight_straight = straight_line_ratio / (straight_line_ratio + 1) if suits_fast_cars == 'Yes' else 0.4\n",
    "    weight_cornering = 1 - weight_straight\n",
    "    track_analysis['StraightZScore'] = zscore(track_analysis['StraightSpeedPotential'])\n",
    "    track_analysis['CorneringZScore'] = -zscore(track_analysis['CorneringPotential'])\n",
    "    track_analysis['OverallFit'] = (weight_straight * track_analysis['StraightZScore'] + weight_cornering * track_analysis['CorneringZScore'])\n",
    "\n",
    "    track_analysis = track_analysis.sort_values('OverallFit', ascending=False)\n",
    "    track_analysis['FitRank'] = track_analysis['OverallFit'].rank(ascending=False)\n",
    "\n",
    "    print(f\"\\nTrack Characteristics Analysis for {event_name}:\")\n",
    "    track_char_df = pd.DataFrame(track_characteristics, index=[0])\n",
    "    display(track_char_df)\n",
    "    print(f\"\\nDriver Fit to Track Characteristics (Top 10) - {event_name}:\")\n",
    "    display(track_analysis[['Driver', 'Team', 'StraightSpeedPotential', 'CorneringPotential', 'AvgSpeed', 'OverallFit', 'FitRank']].head(10))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    scatter = plt.scatter(track_analysis['StraightSpeedPotential'], track_analysis['CorneringPotential'],\n",
    "                          c=track_analysis['FitRank'], cmap='viridis')\n",
    "    plt.colorbar(scatter, label='Fit Rank')\n",
    "    for i, row in track_analysis.iterrows():\n",
    "        plt.text(row['StraightSpeedPotential'], row['CorneringPotential'], row['Driver'], fontsize=8, ha='right', va='bottom')\n",
    "    plt.xlabel('Straight Speed Potential (km/h)')\n",
    "    plt.ylabel('Cornering Potential (seconds)')\n",
    "    plt.title(f'Track Fit Analysis - {year} {event_name} (Round {round_number})')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    scatter = plt.scatter(track_analysis['StraightSpeedPotential'], track_analysis['AvgSpeed'],\n",
    "                          c=track_analysis['FitRank'], cmap='viridis')\n",
    "    plt.colorbar(scatter, label='Fit Rank')\n",
    "    for i, row in track_analysis.iterrows():\n",
    "        plt.text(row['StraightSpeedPotential'], row['AvgSpeed'], row['Driver'], fontsize=8, ha='right', va='bottom')\n",
    "    plt.xlabel('Max Straight Speed (km/h)')\n",
    "    plt.ylabel('Average Speed (km/h)')\n",
    "    plt.title(f'Aeromap - {year} {event_name} (Round {round_number})')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    output_file = os.path.join(data_dir, f\"{year}_R{round_number}_track_characteristics.csv\")\n",
    "    driver_file = os.path.join(data_dir, f\"{year}_R{round_number}_track_characteristics_drivers.csv\")\n",
    "    track_char_df.to_csv(output_file, index=False)\n",
    "    track_analysis.to_csv(driver_file, index=False)\n",
    "    print(f\"Saved track characteristics to {output_file}\")\n",
    "    print(f\"Saved driver fit data to {driver_file}\")\n",
    "    return track_analysis  # Return driver fit data as primary output\n",
    "\n",
    "# Standalone execution for debugging (runs only if cell is executed directly)\n",
    "if 'target_info' in globals() and 'cleaned_data' in globals():\n",
    "    track_char_drivers_df = analyze_track_characteristics(target_info, cleaned_data)\n",
    "else:\n",
    "    print(\"Warning: target_info and/or cleaned_data not found. Run Cells 2 and 4 first for standalone execution.\")"
   ],
   "id": "820d700a6526b105",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 10: Race Strategy and Final Prediction\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import fastf1\n",
    "\n",
    "def predict_race_outcomes(config, driver_performance_df, telemetry_metrics_df, sandbag_analysis_df, track_evolution_df, track_char_drivers_df, pit_stop_loss_default=25, overtaking_penalty=0.5):\n",
    "    year = config.get('year')\n",
    "    round_number = config.get('round')\n",
    "    data_dir = config.get('data_dir', \"raw_data\")\n",
    "    race_laps = config.get('race_laps', 50)\n",
    "    event_name = config.get('event_name', \"Unknown Event\")\n",
    "\n",
    "    BASE_DIR = os.getcwd()\n",
    "    round_data_dir = os.path.join(BASE_DIR, data_dir, str(year), f\"R{round_number}\")\n",
    "    os.makedirs(round_data_dir, exist_ok=True)\n",
    "\n",
    "    # Load dynamic pit_stop_loss\n",
    "    pit_stop_file = os.path.join(BASE_DIR, \"pit_stop_loss_by_track.csv\")\n",
    "    if os.path.exists(pit_stop_file):\n",
    "        pit_stop_df = pd.read_csv(pit_stop_file)\n",
    "        if event_name in pit_stop_df['Track'].values:\n",
    "            pit_stop_loss = pit_stop_df.loc[pit_stop_df['Track'] == event_name, 'PitStopLoss'].values[0]\n",
    "            print(f\"Loaded pit_stop_loss from file: {pit_stop_loss:.2f}s for {event_name}\")\n",
    "        else:\n",
    "            pit_stop_loss = pit_stop_loss_default\n",
    "            print(f\"No pit_stop_loss found for {event_name} in file, using default: {pit_stop_loss}s\")\n",
    "    else:\n",
    "        pit_stop_loss = pit_stop_loss_default\n",
    "        print(f\"Pit stop loss file not found, using default: {pit_stop_loss}s\")\n",
    "\n",
    "    # Handle NaN in driver_performance_df\n",
    "    driver_performance_df['BasePace'] = driver_performance_df['BasePace'].fillna(driver_performance_df['FastestLapTime'] + 5)\n",
    "    driver_performance_df['DegradationSlope'] = driver_performance_df['DegradationSlope'].fillna(7.0)\n",
    "    driver_performance_df['LapTimeVar'] = driver_performance_df['LapTimeVar'].fillna(driver_performance_df['BasePace'] * 0.01)\n",
    "\n",
    "    # Qualifying Prediction\n",
    "    quali_prediction = driver_performance_df[['Driver', 'FastestLapTime', 'Team']].copy()\n",
    "    quali_prediction = quali_prediction.rename(columns={'FastestLapTime': 'LapTime'})\n",
    "    quali_prediction['ExpectedLapTime'] = driver_performance_df['Sector1Time'] + driver_performance_df['Sector2Time'] + driver_performance_df['Sector3Time']\n",
    "    quali_prediction = quali_prediction.merge(sandbag_analysis_df[['Driver', 'LapTimeDiscrepancy', 'SandbagFlag']], on='Driver', how='left')\n",
    "    quali_prediction = quali_prediction.merge(track_char_drivers_df[['Driver', 'OverallFit']], on='Driver', how='left')\n",
    "    quali_prediction['AdjustedLapTime'] = quali_prediction.apply(\n",
    "        lambda row: row['ExpectedLapTime'] if (row.get('SandbagFlag', False) or (row.get('OverallFit', 0) > 0.5 and row['LapTime'] > row['ExpectedLapTime'])) else row['LapTime'], axis=1\n",
    "    )\n",
    "    quali_prediction['QualiPosition'] = quali_prediction['AdjustedLapTime'].rank(method='min').astype(int)\n",
    "\n",
    "    print(\"\\nQualifying Prediction:\")\n",
    "    display(quali_prediction[['Driver', 'Team', 'LapTime', 'ExpectedLapTime', 'AdjustedLapTime', 'QualiPosition']].sort_values('QualiPosition'))\n",
    "\n",
    "    # Race Strategy Simulation\n",
    "    strategy_df = driver_performance_df.copy()\n",
    "    strategy_df['QualiPace'] = quali_prediction['AdjustedLapTime']\n",
    "    strategy_df['GridPosition'] = quali_prediction['QualiPosition']\n",
    "\n",
    "    strategy_df['FuelPenalty'] = strategy_df['DegradationSlope'] * (race_laps / 2) / 100 * 2\n",
    "    strategy_df['TrafficPenalty'] = (strategy_df['LapTimeVar'] / strategy_df['BasePace']) * overtaking_penalty * (race_laps / 50) * 5\n",
    "\n",
    "    strategies = []\n",
    "    for _, row in strategy_df.iterrows():\n",
    "        driver = row['Driver']\n",
    "        team = row['Team']\n",
    "        base_pace = row['BasePace']\n",
    "        deg_rate = row['DegradationSlope'] / 100\n",
    "        fuel_penalty = row['FuelPenalty']\n",
    "        traffic_penalty = row['TrafficPenalty'] if row['GridPosition'] > 5 else row['TrafficPenalty'] * 0.5\n",
    "        grid_pos = row['GridPosition']\n",
    "        quali_pace = row['QualiPace']\n",
    "\n",
    "        start_compound = 'MEDIUM' if grid_pos <= 10 else 'SOFT'\n",
    "        pos_penalty = (grid_pos - 1) * (quali_pace - strategy_df['QualiPace'].min()) / 5\n",
    "        adjusted_pace = base_pace + pos_penalty + fuel_penalty + traffic_penalty\n",
    "\n",
    "        max_stint = 27\n",
    "        if start_compound == 'SOFT':\n",
    "            first_stint = min(max_stint, max(5, 15 / (max(deg_rate * 3, 0.07))))\n",
    "        else:\n",
    "            first_stint = min(max_stint, max(7, 20 / (max(deg_rate * 2, 0.07))))\n",
    "        second_compound = 'HARD'\n",
    "        second_stint = race_laps - first_stint\n",
    "\n",
    "        if first_stint > 7:\n",
    "            first_stint = 7\n",
    "            second_stint = race_laps - first_stint\n",
    "\n",
    "        one_stop_pace = ((adjusted_pace + deg_rate * (first_stint / 2)) * first_stint +\n",
    "                         (adjusted_pace + deg_rate * (second_stint / 2)) * second_stint + pit_stop_loss)\n",
    "        one_stop_stints = [first_stint, second_stint]\n",
    "        one_stop_compounds = [start_compound, second_compound]\n",
    "\n",
    "        if deg_rate * race_laps > 1.0:\n",
    "            first_stint = min(max_stint, race_laps / 3)\n",
    "            second_compound = 'MEDIUM' if start_compound != 'MEDIUM' else 'SOFT'\n",
    "            second_stint_length = min(max_stint, (race_laps - first_stint) / 2)\n",
    "            third_compound = 'HARD'\n",
    "            third_stint = race_laps - first_stint - second_stint_length\n",
    "            if third_stint > 0:\n",
    "                two_stop_pace = ((adjusted_pace + deg_rate * (first_stint / 2)) * first_stint +\n",
    "                                 (adjusted_pace + deg_rate * (second_stint_length / 2)) * second_stint_length +\n",
    "                                 (adjusted_pace + deg_rate * (third_stint / 2)) * third_stint + 2 * pit_stop_loss)\n",
    "                two_stop_stints = [first_stint, second_stint_length, third_stint]\n",
    "                two_stop_compounds = [start_compound, second_compound, third_compound]\n",
    "            else:\n",
    "                two_stop_pace = one_stop_pace + pit_stop_loss\n",
    "                two_stop_stints = one_stop_stints\n",
    "                two_stop_compounds = one_stop_compounds\n",
    "        else:\n",
    "            two_stop_pace = float('inf')\n",
    "            two_stop_stints = []\n",
    "            two_stop_compounds = []\n",
    "\n",
    "        best_time = min(one_stop_pace, two_stop_pace)\n",
    "        strategy = 'One-Stop' if one_stop_pace <= two_stop_pace else 'Two-Stop'\n",
    "        stint_lengths = one_stop_stints if strategy == 'One-Stop' else two_stop_stints\n",
    "        compounds = one_stop_compounds if strategy == 'One-Stop' else two_stop_compounds\n",
    "\n",
    "        strategies.append({\n",
    "            'Driver': driver,\n",
    "            'Team': team,\n",
    "            'GridPosition': grid_pos,\n",
    "            'QualiPace': quali_pace,\n",
    "            'BasePace': base_pace,\n",
    "            'LapTimeVar': row['LapTimeVar'],\n",
    "            'DegradationSlope': row['DegradationSlope'],\n",
    "            'FuelPenalty': fuel_penalty,\n",
    "            'TrafficPenalty': traffic_penalty,\n",
    "            'CompoundStrategy': '-'.join(compounds),\n",
    "            'StintLengths': stint_lengths,\n",
    "            'TotalRaceTime': best_time\n",
    "        })\n",
    "\n",
    "    race_prediction = pd.DataFrame(strategies)\n",
    "\n",
    "    race_prediction = race_prediction.merge(telemetry_metrics_df[['Driver', 'MaxSpeed', 'ThrottleTime']], on='Driver', how='left')\n",
    "    race_prediction = race_prediction.merge(track_char_drivers_df[['Driver', 'OverallFit', 'AvgSpeed']], on='Driver', how='left')\n",
    "\n",
    "    p2_lap_time = track_evolution_df[track_evolution_df['Session'] == 'Practice 2']['AvgLapTime'].iloc[0]\n",
    "    p3_lap_time = track_evolution_df[track_evolution_df['Session'] == 'Practice 3']['AvgLapTime'].iloc[0]\n",
    "    track_evolution_factor = (p3_lap_time - p2_lap_time) / p2_lap_time\n",
    "\n",
    "    chaos_factor = (race_prediction['LapTimeVar'].mean() / race_prediction['BasePace'].mean()) * race_prediction['LapTimeVar'].max() / 20\n",
    "    quali_gaps = race_prediction['QualiPace'].max() - race_prediction['QualiPace'].min()\n",
    "    overtaking_adjustment = (quali_gaps / (race_prediction['GridPosition'].max() - 1)) * 0.2 if race_prediction['GridPosition'].max() > 1 else 0.2\n",
    "    race_load_factor = (race_prediction['BasePace'].mean() - race_prediction['QualiPace'].mean()) / (race_laps / 10) * 1.5\n",
    "\n",
    "    race_prediction['RaceLoad'] = race_load_factor * (race_prediction['LapTimeVar'] / race_prediction['BasePace']) * (race_laps / 2)\n",
    "    race_prediction['TrackEvolutionAdjustment'] = track_evolution_factor * race_prediction['TotalRaceTime'] * (race_prediction['DegradationSlope'] / 100)\n",
    "    race_prediction['ConsistencyAdjustment'] = race_prediction['LapTimeVar'] / race_prediction['BasePace'] * 10\n",
    "    race_prediction['AdjustedRaceTime'] = race_prediction['TotalRaceTime'] + race_prediction['RaceLoad'] + race_prediction['TrackEvolutionAdjustment'] + race_prediction['ConsistencyAdjustment']\n",
    "    race_prediction['SpeedAdjustment'] = (race_prediction['MaxSpeed'] / race_prediction['MaxSpeed'].max()) * 3\n",
    "    race_prediction['FitAdjustment'] = (1 - race_prediction['OverallFit'].rank() / len(race_prediction)) * 10\n",
    "    race_prediction['AvgSpeedAdjustment'] = (race_prediction['AvgSpeed'] / race_prediction['AvgSpeed'].max()) * 20\n",
    "    race_prediction['AdjustedRaceTime'] = race_prediction['AdjustedRaceTime'] - race_prediction['SpeedAdjustment'] - race_prediction['FitAdjustment'] - race_prediction['AvgSpeedAdjustment']\n",
    "\n",
    "    for idx, row in race_prediction.iterrows():\n",
    "        grid_pos = row['GridPosition']\n",
    "        if grid_pos > 10:\n",
    "            race_prediction.at[idx, 'AdjustedRaceTime'] += chaos_factor * (grid_pos - 10) / 10\n",
    "        pos_diff = max(0, grid_pos - 5)\n",
    "        race_prediction.at[idx, 'AdjustedRaceTime'] += pos_diff * overtaking_adjustment\n",
    "\n",
    "    race_prediction['FinalPosition'] = race_prediction['AdjustedRaceTime'].rank(method='min').astype(int)\n",
    "\n",
    "    print(\"\\nFinal Race Prediction:\")\n",
    "    display(race_prediction[['Driver', 'Team', 'GridPosition', 'TotalRaceTime', 'AdjustedRaceTime', 'FinalPosition']].sort_values('FinalPosition'))\n",
    "\n",
    "    # Fastest Lap Prediction\n",
    "    fastest_laps = []\n",
    "    for session_label in ['Practice 1', 'Practice 2', 'Practice 3']:\n",
    "        try:\n",
    "            session = fastf1.get_session(year, round_number, session_label)\n",
    "            session.load(laps=True)\n",
    "            laps = session.laps\n",
    "            for driver in laps['Driver'].unique():\n",
    "                driver_laps = laps[laps['Driver'] == driver]\n",
    "                fastest_lap = driver_laps.loc[driver_laps['LapTime'].idxmin()]\n",
    "                if not pd.isna(fastest_lap['LapTime']):\n",
    "                    fastest_laps.append({\n",
    "                        'Driver': driver,\n",
    "                        'FastestLapTime': fastest_lap['LapTime'].total_seconds(),\n",
    "                        'Session': session_label\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {session_label} for fastest lap prediction: {e}\")\n",
    "            continue\n",
    "\n",
    "    if fastest_laps:\n",
    "        fastest_lap_df = pd.DataFrame(fastest_laps)\n",
    "        fastest_lap_pred = fastest_lap_df.loc[fastest_lap_df['FastestLapTime'].idxmin(), 'Driver']\n",
    "        print(f\"\\nPredicted Fastest Lap Driver: {fastest_lap_pred}\")\n",
    "    else:\n",
    "        fastest_lap_pred = race_prediction.loc[race_prediction['AdjustedRaceTime'].idxmin(), 'Driver']\n",
    "        print(f\"\\nNo fastest lap data available; fallback to race time prediction: {fastest_lap_pred}\")\n",
    "\n",
    "    # Save results\n",
    "    quali_output_file = os.path.join(round_data_dir, f\"{year}_R{round_number}_quali_prediction.csv\")\n",
    "    race_output_file = os.path.join(round_data_dir, f\"{year}_R{round_number}_race_prediction.csv\")\n",
    "    quali_prediction.to_csv(quali_output_file, index=False)\n",
    "    race_prediction.to_csv(race_output_file, index=False)\n",
    "    print(f\"Saved qualifying prediction to {quali_output_file}\")\n",
    "    print(f\"Saved race prediction to {race_output_file}\")\n",
    "\n",
    "    return quali_prediction, race_prediction, fastest_lap_pred\n",
    "\n",
    "# Standalone execution for debugging (runs only if required DataFrames are in scope)\n",
    "if all(var in globals() for var in ['target_info', 'driver_performance_df', 'telemetry_metrics_df', 'sandbag_analysis_df', 'track_evolution_df', 'track_char_drivers_df']):\n",
    "    quali_prediction_df, race_prediction_df, fastest_lap_pred = predict_race_outcomes(\n",
    "        target_info, driver_performance_df, telemetry_metrics_df, sandbag_analysis_df, track_evolution_df, track_char_drivers_df\n",
    "    )\n",
    "else:\n",
    "    print(\"Warning: Required variables (target_info, driver_performance_df, telemetry_metrics_df, sandbag_analysis_df, track_evolution_df, track_char_drivers_df) not found. Run Cells 2, 5, 6, 7, 8, and 9 first for standalone execution.\")"
   ],
   "id": "78072ef9ab25f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 10.5: Comprehensive Pre-Qualifying Report\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from fpdf import FPDF\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "def generate_comprehensive_report(config, driver_performance_df, telemetry_metrics_df, sandbag_analysis_df, track_evolution_df, track_char_drivers_df, quali_prediction_df, race_prediction_df, fastest_lap_pred):\n",
    "    year = config.get('year')\n",
    "    round_number = config.get('round')\n",
    "    event_name = config.get('event_name', \"Unknown Event\")\n",
    "    data_dir = os.path.join(\"raw_data\", str(year), f\"R{round_number}\")\n",
    "    BASE_DIR = os.getcwd()\n",
    "    cleaned_data_dir = os.path.join(BASE_DIR, data_dir, \"cleaned_data\")\n",
    "    report_dir = os.path.join(BASE_DIR, \"reports\", str(year), f\"R{round_number}\")\n",
    "    os.makedirs(report_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize PDF\n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", \"B\", 16)\n",
    "    pdf.cell(0, 10, f\"Pre-Qualifying Report: {year} Round {round_number} - {event_name}\", ln=True, align=\"C\")\n",
    "    pdf.ln(10)\n",
    "\n",
    "    # Helper function to add DataFrame to PDF\n",
    "    def add_df_to_pdf(df, title):\n",
    "        pdf.set_font(\"Arial\", \"B\", 12)\n",
    "        pdf.cell(0, 10, title, ln=True)\n",
    "        pdf.set_font(\"Arial\", size=10)\n",
    "        col_widths = [max([len(str(x)) for x in df[col].values] + [len(col)]) * 5 for col in df.columns]\n",
    "        total_width = sum(col_widths)\n",
    "        if total_width > 190:\n",
    "            scale_factor = 190 / total_width\n",
    "            col_widths = [w * scale_factor for w in col_widths]\n",
    "        for i, row in df.iterrows():\n",
    "            for j, val in enumerate(row):\n",
    "                pdf.cell(col_widths[j], 10, str(val)[:20], border=1)\n",
    "            pdf.ln()\n",
    "        pdf.ln(5)\n",
    "\n",
    "    # Helper function to add plot to PDF\n",
    "    def add_plot_to_pdf(fig, title):\n",
    "        pdf.set_font(\"Arial\", \"B\", 12)\n",
    "        pdf.cell(0, 10, title, ln=True)\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        temp_file = os.path.join(temp_dir, f\"{title.replace(' ', '_')}.png\")\n",
    "        try:\n",
    "            fig.savefig(temp_file, format='png', bbox_inches='tight')\n",
    "            pdf.image(temp_file, x=10, w=190)\n",
    "        finally:\n",
    "            plt.close(fig)\n",
    "            shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "        pdf.ln(5)\n",
    "\n",
    "    # Step 1: Event Info\n",
    "    print(\"--- Step 1: Event Information ---\")\n",
    "    pdf.set_font(\"Arial\", \"B\", 12)\n",
    "    pdf.cell(0, 10, \"Step 1: Event Information\", ln=True)\n",
    "    info_text = (\n",
    "        f\"Year: {year}, Round: {round_number}, Event: {event_name}\\n\"\n",
    "        f\"Track Length: {config.get('track_length', 'N/A')} km\\n\"\n",
    "        f\"Average Speed: {config.get('avg_speed', 'N/A')} km/h\\n\"\n",
    "        f\"Downforce Level: {config.get('downforce_level', 'N/A')}\"\n",
    "    )\n",
    "    print(info_text)\n",
    "    pdf.set_font(\"Arial\", size=10)\n",
    "    pdf.multi_cell(0, 10, info_text)\n",
    "    pdf.ln(5)\n",
    "\n",
    "    # Step 2: Historical Data\n",
    "    print(\"--- Step 2: Historical Data ---\")\n",
    "    pdf.set_font(\"Arial\", \"B\", 12)\n",
    "    pdf.cell(0, 10, \"Step 2: Historical Data\", ln=True)\n",
    "    hist_text = (\n",
    "        f\"Historical Winners: {config.get('historical_winners', 'N/A')}\\n\"\n",
    "        f\"Historical Poles: {config.get('historical_poles', 'N/A')}\"\n",
    "    )\n",
    "    print(hist_text)\n",
    "    pdf.set_font(\"Arial\", size=10)\n",
    "    pdf.multi_cell(0, 10, hist_text)\n",
    "    pdf.ln(5)\n",
    "\n",
    "    # Step 3: Session Data\n",
    "    print(\"--- Step 3: Session Data ---\")\n",
    "    session_counts = {}\n",
    "    for session in ['Practice 1', 'Practice 2', 'Practice 3']:\n",
    "        try:\n",
    "            session_file = os.path.join(BASE_DIR, data_dir, f\"{year}_R{round_number}_{session.replace(' ', '_')}_laps.csv\")\n",
    "            session_data = pd.read_csv(session_file)\n",
    "            session_counts[session] = len(session_data)\n",
    "            print(f\"{session} Lap Count: {session_counts[session]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {session} data: {e}\")\n",
    "    pdf.set_font(\"Arial\", \"B\", 12)\n",
    "    pdf.cell(0, 10, \"Step 3: Session Data\", ln=True)\n",
    "    pdf.set_font(\"Arial\", size=10)\n",
    "    for session, count in session_counts.items():\n",
    "        pdf.cell(0, 10, f\"{session} Lap Count: {count}\", ln=True)\n",
    "    pdf.ln(5)\n",
    "    print()\n",
    "\n",
    "    # Step 4: Data Cleaning Summary\n",
    "    print(\"--- Step 4: Data Cleaning Summary ---\")\n",
    "    cleaned_counts = {}\n",
    "    cleaned_data_files = [f\"{year}_R{round_number}_Practice_{i}_cleaned_laps.csv\" for i in range(1, 4)]\n",
    "    for file in cleaned_data_files:\n",
    "        try:\n",
    "            cleaned_data = pd.read_csv(os.path.join(cleaned_data_dir, file))\n",
    "            cleaned_counts[file] = len(cleaned_data)\n",
    "            print(f\"Cleaned {file}: {cleaned_counts[file]} laps\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "    pdf.set_font(\"Arial\", \"B\", 12)\n",
    "    pdf.cell(0, 10, \"Step 4: Data Cleaning Summary\", ln=True)\n",
    "    pdf.set_font(\"Arial\", size=10)\n",
    "    for file, count in cleaned_counts.items():\n",
    "        pdf.cell(0, 10, f\"Cleaned {file}: {count} laps\", ln=True)\n",
    "    pdf.ln(5)\n",
    "    print()\n",
    "\n",
    "    # Step 5: Driver Performance\n",
    "    print(\"--- Step 5: Driver Performance ---\")\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", \"B\", 12)\n",
    "    pdf.cell(0, 10, \"Step 5: Driver Performance\", ln=True)\n",
    "    df = driver_performance_df[['Driver', 'FastestLapTime', 'TheoreticalLapTime', 'BasePace', 'DegradationSlope', 'Team']]\n",
    "    print(\"Top 5 by Fastest Lap Time:\")\n",
    "    display(df.sort_values('FastestLapTime').head(5))\n",
    "    add_df_to_pdf(df.sort_values('FastestLapTime').head(5), \"Top 5 by Fastest Lap Time\")\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    bar_width = 0.35\n",
    "    index = range(5)\n",
    "    ax.bar([i - bar_width/2 for i in index], df['FastestLapTime'].head(5), bar_width, label='Fastest Lap Time', color='blue')\n",
    "    ax.bar([i + bar_width/2 for i in index], df['TheoreticalLapTime'].head(5), bar_width, label='Theoretical Lap Time', color='orange')\n",
    "    ax.set_xlabel('Driver')\n",
    "    ax.set_ylabel('Lap Time (seconds)')\n",
    "    ax.set_title(f'Fastest vs. Theoretical Lap Times ({year} Round {round_number})')\n",
    "    ax.set_xticks(index)\n",
    "    ax.set_xticklabels(df['Driver'].head(5), rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    add_plot_to_pdf(fig, \"Fastest vs. Theoretical Lap Times\")\n",
    "    print()\n",
    "\n",
    "    # Step 6: Telemetry Metrics\n",
    "    print(\"--- Step 6: Telemetry Metrics ---\")\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", \"B\", 12)\n",
    "    pdf.cell(0, 10, \"Step 6: Telemetry Metrics\", ln=True)\n",
    "    df = telemetry_metrics_df[['Driver', 'MaxSpeed', 'ThrottleTime', 'BrakingIntensity', 'Team']]\n",
    "    print(\"Top 5 by Max Speed:\")\n",
    "    display(df.sort_values('MaxSpeed', ascending=False).head(5))\n",
    "    add_df_to_pdf(df.sort_values('MaxSpeed', ascending=False).head(5), \"Top 5 by Max Speed\")\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(12, 8))\n",
    "    metrics = ['MaxSpeed', 'ThrottleTime', 'BrakingIntensity']\n",
    "    titles = ['Max Speed (km/h)', '% Time at Full Throttle', 'Avg Braking Intensity (0-1)']\n",
    "    colors = ['blue', 'orange', 'green']\n",
    "    for i, (metric, title, color) in enumerate(zip(metrics, titles, colors)):\n",
    "        plot_data = df[df[metric] > 0] if metric == 'ThrottleTime' else df\n",
    "        sorted_data = plot_data.sort_values(metric, ascending=False).head(5)\n",
    "        axs[i].bar(sorted_data['Driver'], sorted_data[metric], color=color)\n",
    "        axs[i].set_title(f'Telemetry Metrics - {title}')\n",
    "        axs[i].set_xlabel('Driver')\n",
    "        axs[i].set_ylabel(title)\n",
    "        axs[i].tick_params(axis='x', rotation=45)\n",
    "        axs[i].grid(True)\n",
    "    plt.tight_layout()\n",
    "    add_plot_to_pdf(fig, \"Telemetry Metrics\")\n",
    "    print()\n",
    "\n",
    "    # Step 7: Sandbagging Analysis\n",
    "    print(\"--- Step 7: Sandbagging Analysis ---\")\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", \"B\", 12)\n",
    "    pdf.cell(0, 10, \"Step 7: Sandbagging Analysis\", ln=True)\n",
    "    df = sandbag_analysis_df[['Driver', 'Team', 'FastestLapTime', 'MaxSpeed', 'PredictionDiscrepancy', 'SandbagFlag']]\n",
    "    print(\"Potential Sandbaggers:\")\n",
    "    display(df[df['SandbagFlag']].sort_values('PredictionDiscrepancy', ascending=False))\n",
    "    add_df_to_pdf(df[df['SandbagFlag']].sort_values('PredictionDiscrepancy', ascending=False), \"Potential Sandbaggers\")\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    colors = ['blue' if not flag else 'red' for flag in df['SandbagFlag']]\n",
    "    ax.scatter(df['FastestLapTime'], df['MaxSpeed'], c=colors, alpha=0.6)\n",
    "    ax.axhline(y=df['MaxSpeed'].mean(), color='gray', linestyle='--', label='Mean Max Speed')\n",
    "    ax.set_xlabel('Fastest Lap Time (seconds)')\n",
    "    ax.set_ylabel('Max Speed (km/h)')\n",
    "    ax.set_title(f'Sandbagging Analysis ({year} Round {round_number})')\n",
    "    for i, row in df.iterrows():\n",
    "        ax.text(row['FastestLapTime'], row['MaxSpeed'], row['Driver'], fontsize=8, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    add_plot_to_pdf(fig, \"Sandbagging Analysis: Fastest Lap Time vs. Max Speed\")\n",
    "    print()\n",
    "\n",
    "    # Step 8: Track Evolution and Weather\n",
    "    print(\"--- Step 8: Track Evolution and Weather ---\")\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", \"B\", 12)\n",
    "    pdf.cell(0, 10, \"Step 8: Track Evolution and Weather\", ln=True)\n",
    "    df = track_evolution_df[['Session', 'AvgLapTime', 'StdLapTime', 'LapCount']]\n",
    "    print(\"Average Lap Times Across Sessions:\")\n",
    "    display(df)\n",
    "    add_df_to_pdf(df, \"Average Lap Times Across Sessions\")\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    ax1.plot(df['Session'], df['AvgLapTime'], marker='o', color='blue', label='Avg Lap Time (s)')\n",
    "    ax1.fill_between(df['Session'], df['AvgLapTime'] - df['StdLapTime'] / 2, df['AvgLapTime'] + df['StdLapTime'] / 2, color='blue', alpha=0.2, label='Std Dev (±0.5σ)')\n",
    "    ax1.set_xlabel('Session')\n",
    "    ax1.set_ylabel('Average Lap Time (seconds)', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    ax1.set_xticks(df['Session'])\n",
    "    ax1.set_xticklabels(['P1', 'P2', 'P3'], rotation=45)\n",
    "    ax1.grid(True)\n",
    "    ax1.legend(loc='upper left')\n",
    "    add_plot_to_pdf(fig, \"Track Evolution: Average Lap Times\")\n",
    "    print()\n",
    "\n",
    "    # Step 9: Track Characteristics\n",
    "    print(\"--- Step 9: Track Characteristics ---\")\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", \"B\", 12)\n",
    "    pdf.cell(0, 10, \"Step 9: Track Characteristics\", ln=True)\n",
    "    track_char_df = pd.read_csv(os.path.join(BASE_DIR, data_dir, f\"{year}_R{round_number}_track_characteristics.csv\"))\n",
    "    print(\"Track Characteristics:\")\n",
    "    display(track_char_df)\n",
    "    add_df_to_pdf(track_char_df, \"Track Characteristics\")\n",
    "    df = track_char_drivers_df[['Driver', 'Team', 'StraightSpeedPotential', 'CorneringPotential', 'AvgSpeed', 'OverallFit', 'FitRank']]\n",
    "    print(\"Top 5 Drivers by Track Fit:\")\n",
    "    display(df.sort_values('FitRank').head(5))\n",
    "    add_df_to_pdf(df.sort_values('FitRank').head(5), \"Top 5 Drivers by Track Fit\")\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    scatter = ax.scatter(df['StraightSpeedPotential'], df['AvgSpeed'], c=df['FitRank'], cmap='viridis')\n",
    "    plt.colorbar(scatter, label='Fit Rank')\n",
    "    for i, row in df.iterrows():\n",
    "        ax.text(row['StraightSpeedPotential'], row['AvgSpeed'], row['Driver'], fontsize=8, ha='right')\n",
    "    ax.set_xlabel('Max Straight Speed (km/h)')\n",
    "    ax.set_ylabel('Average Speed (km/h)')\n",
    "    ax.set_title(f'Aeromap ({year} Round {round_number})')\n",
    "    ax.grid(True)\n",
    "    add_plot_to_pdf(fig, \"Aeromap: Max Speed vs. Average Speed\")\n",
    "    print()\n",
    "\n",
    "    # Step 10: Predictions\n",
    "    print(\"--- Step 10: Predictions ---\")\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", \"B\", 12)\n",
    "    pdf.cell(0, 10, \"Step 10: Predictions\", ln=True)\n",
    "    quali_df = quali_prediction_df[['Driver', 'Team', 'AdjustedLapTime', 'QualiPosition']]\n",
    "    print(\"Predicted Qualifying Top 5:\")\n",
    "    display(quali_df.sort_values('QualiPosition').head(5))\n",
    "    add_df_to_pdf(quali_df.sort_values('QualiPosition').head(5), \"Predicted Qualifying Top 5\")\n",
    "    race_df = race_prediction_df[['Driver', 'Team', 'GridPosition', 'AdjustedRaceTime', 'FinalPosition']]\n",
    "    print(\"Predicted Race Top 10:\")\n",
    "    display(race_df.sort_values('FinalPosition').head(10))\n",
    "    add_df_to_pdf(race_df.sort_values('FinalPosition').head(10), \"Predicted Race Top 10\")\n",
    "    pdf.set_font(\"Arial\", size=10)\n",
    "    pdf.cell(0, 10, f\"Predicted Fastest Lap Driver: {fastest_lap_pred}\", ln=True)\n",
    "    print(f\"Predicted Fastest Lap Driver: {fastest_lap_pred}\\n\")\n",
    "\n",
    "    # Save PDF\n",
    "    pdf_output = os.path.join(report_dir, f\"{year}_R{round_number}_pre_qualifying_report.pdf\")\n",
    "    pdf.output(pdf_output)\n",
    "    print(f\"Report saved to {pdf_output}\")\n",
    "\n",
    "# Standalone execution for debugging (runs only if required variables are in scope)\n",
    "if all(var in globals() for var in ['target_info', 'driver_performance_df', 'telemetry_metrics_df', 'sandbag_analysis_df', 'track_evolution_df', 'track_char_drivers_df', 'quali_prediction_df', 'race_prediction_df', 'fastest_lap_pred']):\n",
    "    generate_comprehensive_report(\n",
    "        target_info, driver_performance_df, telemetry_metrics_df, sandbag_analysis_df,\n",
    "        track_evolution_df, track_char_drivers_df, quali_prediction_df, race_prediction_df, fastest_lap_pred\n",
    "    )\n",
    "else:\n",
    "    print(\"Warning: Required variables (target_info, driver_performance_df, telemetry_metrics_df, sandbag_analysis_df, track_evolution_df, track_char_drivers_df, quali_prediction_df, race_prediction_df, fastest_lap_pred) not found. Run Cells 2, 5, 6, 7, 8, 9, and 10 first for standalone execution.\")"
   ],
   "id": "a72d916b3d63c840",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 11: Final Summary Report\n",
    "import pandas as pd\n",
    "import os\n",
    "import fastf1\n",
    "import numpy as np\n",
    "\n",
    "def generate_summary_report(config, quali_prediction_df, race_prediction_df, fastest_lap_pred):\n",
    "    year = config.get('year')\n",
    "    round_number = config.get('round')\n",
    "    data_dir = os.path.join(\"raw_data\", str(year), f\"R{round_number}\")\n",
    "    event_name = config.get('event_name', \"Unknown Event\")\n",
    "    BASE_DIR = os.getcwd()\n",
    "\n",
    "    print(\"Debug: Verifying input DataFrames...\")\n",
    "    print(f\"quali_prediction_df shape: {quali_prediction_df.shape if quali_prediction_df is not None else 'None'}\")\n",
    "    print(f\"race_prediction_df shape: {race_prediction_df.shape if race_prediction_df is not None else 'None'}\")\n",
    "\n",
    "    round_data_dir = os.path.join(BASE_DIR, data_dir)\n",
    "    driver_performance_file = os.path.join(round_data_dir, f\"{year}_R{round_number}_driver_performance.csv\")\n",
    "    try:\n",
    "        driver_performance = pd.read_csv(driver_performance_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading driver performance data: {e}\")\n",
    "        driver_performance = pd.DataFrame()\n",
    "\n",
    "    # Load pit_stop_loss\n",
    "    pit_stop_file = os.path.join(BASE_DIR, \"pit_stop_loss_by_track.csv\")\n",
    "    if os.path.exists(pit_stop_file):\n",
    "        pit_stop_df = pd.read_csv(pit_stop_file)\n",
    "        if event_name in pit_stop_df['Track'].values:\n",
    "            pit_stop_loss = pit_stop_df.loc[pit_stop_df['Track'] == event_name, 'PitStopLoss'].values[0]\n",
    "            print(f\"Loaded pit_stop_loss for comparison: {pit_stop_loss:.2f}s for {event_name}\")\n",
    "        else:\n",
    "            pit_stop_loss = 20.0\n",
    "            print(f\"No pit_stop_loss found for {event_name} in file, using default: {pit_stop_loss}s\")\n",
    "    else:\n",
    "        pit_stop_loss = 20.0\n",
    "        print(f\"Pit stop loss file not found, using default: {pit_stop_loss}s\")\n",
    "\n",
    "    # Fetch actual results\n",
    "    try:\n",
    "        quali_session = fastf1.get_session(year, round_number, 'Q')\n",
    "        quali_session.load()\n",
    "        race_session = fastf1.get_session(year, round_number, 'R')\n",
    "        race_session.load()\n",
    "        quali_results = quali_session.results\n",
    "        race_results = race_session.results\n",
    "        actual_quali_top_5 = quali_results['Abbreviation'].head(5).tolist()\n",
    "        actual_race_top_10 = race_results['Abbreviation'].head(10).tolist()\n",
    "        laps = race_session.laps\n",
    "        fastest_lap_driver = laps.pick_fastest()['Driver']\n",
    "\n",
    "        actual_race_pace = laps[laps['PitInTime'].isna() & laps['LapTime'].notna()].groupby('Driver')['LapTime'].mean().dt.total_seconds().reset_index()\n",
    "        actual_race_pace.columns = ['Driver', 'ActualRacePace']\n",
    "\n",
    "        actual_deg = laps[laps['TyreLife'].notna() & laps['LapTime'].notna()].groupby('Driver').apply(\n",
    "            lambda x: np.polyfit(x['TyreLife'], x['LapTime'].dt.total_seconds(), 1)[0] * 100 if len(x) > 5 else np.nan,\n",
    "            include_groups=False\n",
    "        ).reset_index()\n",
    "        actual_deg.columns = ['Driver', 'ActualDegradationSlope']\n",
    "\n",
    "        actual_overtakes = race_results[['Abbreviation', 'GridPosition', 'Position']].dropna()\n",
    "        actual_overtakes['Position'] = actual_overtakes['Position'].astype(int)\n",
    "        actual_overtakes['Overtakes'] = actual_overtakes['GridPosition'] - actual_overtakes['Position']\n",
    "        actual_overtakes = actual_overtakes[['Abbreviation', 'Overtakes']]\n",
    "        actual_overtakes.columns = ['Driver', 'ActualOvertakes']\n",
    "\n",
    "        pit_in_laps = laps[laps['PitInTime'].notna()].copy()\n",
    "        if not pit_in_laps.empty:\n",
    "            avg_clean_lap = actual_race_pace.set_index('Driver')['ActualRacePace']\n",
    "            pit_in_laps['PitDuration'] = pit_in_laps.apply(\n",
    "                lambda row: row['LapTime'].total_seconds() - avg_clean_lap.get(row['Driver'], row['LapTime'].total_seconds()) if pd.notna(row['LapTime']) else np.nan,\n",
    "                axis=1\n",
    "            )\n",
    "            pit_stops = pit_in_laps.groupby('Driver').agg(\n",
    "                NumPitStops=pd.NamedAgg(column='PitInTime', aggfunc='count'),\n",
    "                TotalPitTime=pd.NamedAgg(column='PitDuration', aggfunc='sum'),\n",
    "                AvgPitTime=pd.NamedAgg(column='PitDuration', aggfunc='mean')\n",
    "            ).reset_index()\n",
    "            print(\"Debug: Sample pit stop durations:\")\n",
    "            print(pit_in_laps[['Driver', 'LapNumber', 'LapTime', 'PitDuration']].head())\n",
    "        else:\n",
    "            print(\"Warning: No pit-in laps found. Pit stop data unavailable.\")\n",
    "            pit_stops = pd.DataFrame(columns=['Driver', 'NumPitStops', 'TotalPitTime', 'AvgPitTime'])\n",
    "\n",
    "        actual_tyres = laps.groupby('Driver')['Compound'].unique().reset_index()\n",
    "        actual_tyres['ActualTyreStrategy'] = actual_tyres['Compound'].apply(lambda x: '-'.join(x))\n",
    "\n",
    "        track_pit_stop_loss = pit_stops['AvgPitTime'].mean() if not pit_stops.empty else 20.0\n",
    "        print(f\"Calculated pit_stop_loss for {event_name}: {track_pit_stop_loss:.2f}s\")\n",
    "        if os.path.exists(pit_stop_file):\n",
    "            pit_stop_df = pd.read_csv(pit_stop_file)\n",
    "            if event_name in pit_stop_df['Track'].values:\n",
    "                pit_stop_df.loc[pit_stop_df['Track'] == event_name, 'PitStopLoss'] = track_pit_stop_loss\n",
    "            else:\n",
    "                pit_stop_df = pd.concat([pit_stop_df, pd.DataFrame({'Track': [event_name], 'PitStopLoss': [track_pit_stop_loss]})], ignore_index=True)\n",
    "        else:\n",
    "            pit_stop_df = pd.DataFrame({'Track': [event_name], 'PitStopLoss': [track_pit_stop_loss]})\n",
    "        pit_stop_df.to_csv(pit_stop_file, index=False)\n",
    "        print(f\"Saved pit_stop_loss to {pit_stop_file}\")\n",
    "\n",
    "        print(f\"Dynamically fetched actual qualifying top 5: {actual_quali_top_5}\")\n",
    "        print(f\"Dynamically fetched actual race top 10: {actual_race_top_10}\")\n",
    "        print(f\"Dynamically fetched actual fastest lap driver: {fastest_lap_driver}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching actual results from FastF1: {e}\")\n",
    "        print(\"Falling back to placeholder results.\")\n",
    "        actual_quali_top_5 = ['Unknown'] * 5\n",
    "        actual_race_top_10 = ['Unknown'] * 10\n",
    "        fastest_lap_driver = 'Unknown'\n",
    "        actual_race_pace = pd.DataFrame()\n",
    "        actual_deg = pd.DataFrame()\n",
    "        actual_overtakes = pd.DataFrame()\n",
    "        pit_stops = pd.DataFrame()\n",
    "        actual_tyres = pd.DataFrame()\n",
    "\n",
    "    print(\"\\n=== Comprehensive Summary Report ===\")\n",
    "    print(f\"\\nEvent: {year} Round {round_number} - {event_name}\")\n",
    "\n",
    "    print(\"\\n--- Qualifying Analysis ---\")\n",
    "    print(\"Actual Qualifying Top 5:\", actual_quali_top_5)\n",
    "    print(\"Predicted Qualifying Top 5:\")\n",
    "    quali_pred_df = quali_prediction_df[['Driver', 'AdjustedLapTime', 'QualiPosition']].sort_values('QualiPosition').head(5)\n",
    "    display(quali_pred_df)\n",
    "\n",
    "    print(\"\\n--- Race Analysis ---\")\n",
    "    print(\"Actual Race Top 10:\", actual_race_top_10)\n",
    "    print(\"Predicted Race Top 10:\")\n",
    "    race_pred_df = race_prediction_df[['Driver', 'Team', 'GridPosition', 'AdjustedRaceTime', 'FinalPosition']].sort_values('FinalPosition').head(10)\n",
    "    display(race_pred_df)\n",
    "\n",
    "    print(\"\\n--- Detailed Prediction vs. Actual Comparison ---\")\n",
    "    comparison_df = race_prediction_df[['Driver', 'GridPosition', 'FinalPosition', 'BasePace', 'DegradationSlope', 'FuelPenalty', 'TrafficPenalty', 'CompoundStrategy', 'StintLengths']].merge(\n",
    "        actual_race_pace, on='Driver', how='left'\n",
    "    ).merge(\n",
    "        actual_deg, on='Driver', how='left'\n",
    "    ).merge(\n",
    "        actual_overtakes, on='Driver', how='left'\n",
    "    ).merge(\n",
    "        pit_stops, on='Driver', how='left'\n",
    "    ).merge(\n",
    "        actual_tyres[['Driver', 'ActualTyreStrategy']], on='Driver', how='left'\n",
    "    )\n",
    "    comparison_df['PaceError'] = comparison_df['BasePace'] - comparison_df['ActualRacePace']\n",
    "    comparison_df['DegError'] = comparison_df['DegradationSlope'] - comparison_df['ActualDegradationSlope']\n",
    "    comparison_df['PositionError'] = comparison_df['FinalPosition'] - comparison_df['ActualOvertakes'].fillna(0).apply(lambda x: len(actual_race_top_10) + 1 - x if x > 0 else len(actual_race_top_10) + 1)\n",
    "    comparison_df['PitStopCountError'] = comparison_df['NumPitStops'].fillna(0) - comparison_df['CompoundStrategy'].apply(lambda x: len(x.split('-')) - 1)\n",
    "    comparison_df['PitTimeError'] = (comparison_df['NumPitStops'].fillna(0) * pit_stop_loss) - comparison_df['TotalPitTime'].fillna(0)\n",
    "    display(comparison_df)\n",
    "\n",
    "    results_file = os.path.join(round_data_dir, f\"{year}_R{round_number}_summary_results.csv\")\n",
    "    comparison_df.to_csv(results_file, index=False)\n",
    "    print(f\"\\nSaved detailed results to {results_file}\")\n",
    "\n",
    "    print(\"\\n--- Insights for Improvement ---\")\n",
    "    print(\"1. **Race Pace Errors**:\")\n",
    "    pace_error_mean = comparison_df['PaceError'].mean()\n",
    "    print(f\"Mean Pace Error: {pace_error_mean:.2f} seconds\")\n",
    "    if abs(pace_error_mean) > 2:\n",
    "        print(f\" - Suggestion: Adjust BasePace calculation. Current model {'overestimates' if pace_error_mean > 0 else 'underestimates'} race pace by {abs(pace_error_mean):.2f}s.\")\n",
    "\n",
    "    print(\"2. **Degradation Errors**:\")\n",
    "    deg_error_mean = comparison_df['DegError'].mean()\n",
    "    print(f\"Mean Degradation Error: {deg_error_mean:.2f}%\")\n",
    "    if abs(deg_error_mean) > 5:\n",
    "        print(f\" - Suggestion: Refine DegradationSlope estimation. Model {'overestimates' if deg_error_mean > 0 else 'underestimates'} tyre wear by {abs(deg_error_mean):.2f}%.\")\n",
    "\n",
    "    print(\"3. **Overtaking Accuracy**:\")\n",
    "    overtake_error_mean = comparison_df['ActualOvertakes'].fillna(0) - (comparison_df['GridPosition'] - comparison_df['FinalPosition'])\n",
    "    print(f\"Mean Overtaking Error: {overtake_error_mean.mean():.2f} positions\")\n",
    "    if abs(overtake_error_mean.mean()) > 2:\n",
    "        print(f\" - Suggestion: Adjust overtaking_penalty (0.5s) or TrafficPenalty. Predictions {'underestimate' if overtake_error_mean.mean() > 0 else 'overestimate'} overtaking by {abs(overtake_error_mean.mean()):.2f} positions.\")\n",
    "\n",
    "    print(\"4. **Pit Stop Strategy**:\")\n",
    "    pit_count_error_mean = comparison_df['PitStopCountError'].mean()\n",
    "    print(f\"Mean Pit Stop Count Error: {pit_count_error_mean:.2f} stops\")\n",
    "    if abs(pit_count_error_mean) > 0.5:\n",
    "        print(f\" - Suggestion: Refine stint length logic. Model {'overestimates' if pit_count_error_mean < 0 else 'underestimates'} pit stops by {abs(pit_count_error_mean):.2f}.\")\n",
    "\n",
    "    print(\"5. **Pit Stop Time**:\")\n",
    "    pit_time_error_mean = comparison_df['PitTimeError'].mean()\n",
    "    print(f\"Mean Pit Time Error: {pit_time_error_mean:.2f} seconds\")\n",
    "    if abs(pit_time_error_mean) > 5:\n",
    "        print(f\" - Suggestion: Adjust pit_stop_loss ({pit_stop_loss:.2f}s). Actual pit times are {'shorter' if pit_time_error_mean > 0 else 'longer'} by {abs(pit_time_error_mean):.2f}s.\")\n",
    "\n",
    "    print(\"\\n--- Driver Performance Metrics ---\")\n",
    "    print(\"Fastest Lap Times:\")\n",
    "    display(driver_performance[['Driver', 'FastestLapTime', 'Team']].sort_values('FastestLapTime'))\n",
    "\n",
    "    print(\"\\n--- F1 Fantasy League Scoring ---\")\n",
    "    quali_top_5_pred = quali_prediction_df.sort_values('QualiPosition').head(5)['Driver'].tolist()\n",
    "    race_top_10_pred = race_prediction_df.sort_values('FinalPosition').head(10)['Driver'].tolist()\n",
    "    podium_pred = race_top_10_pred[:3]\n",
    "    podium_actual = actual_race_top_10[:3]\n",
    "\n",
    "    podium_points = 0\n",
    "    podium_details = []\n",
    "    for position, (pred, act) in enumerate(zip(podium_pred, podium_actual), 1):\n",
    "        points = 0\n",
    "        if pred in actual_race_top_10:\n",
    "            points += 1\n",
    "            podium_details.append(f\"P{position}           {act:<15} {pred:<15} 1pt (in top 10)\")\n",
    "        if pred == act:\n",
    "            points += 1\n",
    "            podium_details.append(f\"P{position}           {act:<15} {pred:<15} 1pt (exact position)\")\n",
    "        if points == 0:\n",
    "            podium_details.append(f\"P{position}           {act:<15} {pred:<15} 0pt\")\n",
    "        podium_points += points\n",
    "    if set(podium_pred).issubset(set(podium_actual)):\n",
    "        podium_points += 2\n",
    "        podium_details.append(f\"Podium Bonus  {str(podium_actual):<15} {str(podium_pred):<15} 2pt (all 3 correct)\")\n",
    "    else:\n",
    "        podium_details.append(f\"Podium Bonus  {str(podium_actual):<15} {str(podium_pred):<15} 0pt (not all 3 correct)\")\n",
    "\n",
    "    top_10_points = 0\n",
    "    top_10_details = []\n",
    "    for position, (pred, act) in enumerate(zip(race_top_10_pred[3:], actual_race_top_10[3:]), 4):\n",
    "        points = 0\n",
    "        if pred in actual_race_top_10:\n",
    "            points += 1\n",
    "            top_10_details.append(f\"P{position}           {act:<15} {pred:<15} 1pt (in top 10)\")\n",
    "            if pred == act:\n",
    "                points += 0.5\n",
    "                top_10_details.append(f\"P{position}           {act:<15} {pred:<15} 0.5pt (exact position)\")\n",
    "        else:\n",
    "            top_10_details.append(f\"P{position}           {act:<15} {pred:<15} 0pt\")\n",
    "        top_10_points += points\n",
    "\n",
    "    fastest_lap_points = 1 if fastest_lap_pred == fastest_lap_driver else 0\n",
    "    fastest_lap_detail = f\"Fastest Lap   {fastest_lap_driver:<15} {fastest_lap_pred:<15} {fastest_lap_points}pt\"\n",
    "\n",
    "    total_points = podium_points + top_10_points + fastest_lap_points\n",
    "\n",
    "    print(\"\\nRace Results\")\n",
    "    print(f\"{'Actual':<15} {'You':<15} {'Points':<15}\")\n",
    "    print(\"-\" * 45)\n",
    "    pole_pred = quali_prediction_df.sort_values('QualiPosition').head(1)['Driver'].tolist()[0]\n",
    "    print(f\"Pole          {actual_quali_top_5[0]:<15} {pole_pred:<15} 2pt\" if pole_pred == actual_quali_top_5[0] else f\"Pole          {actual_quali_top_5[0]:<15} {pole_pred:<15} 0pt\")\n",
    "    print(fastest_lap_detail)\n",
    "    for detail in podium_details:\n",
    "        print(detail)\n",
    "    for detail in top_10_details:\n",
    "        print(detail)\n",
    "\n",
    "    print(f\"\\nTotal Fantasy League Points: {total_points}\")\n",
    "    print(f\"Maximum Possible Points: 20\")\n",
    "\n",
    "# Standalone execution for debugging (runs only if required variables are in scope)\n",
    "if all(var in globals() for var in ['target_info', 'quali_prediction_df', 'race_prediction_df', 'fastest_lap_pred']):\n",
    "    generate_summary_report(target_info, quali_prediction_df, race_prediction_df, fastest_lap_pred)\n",
    "else:\n",
    "    print(\"Warning: Required variables (target_info, quali_prediction_df, race_prediction_df, fastest_lap_pred) not found. Run Cells 2 and 10 first for standalone execution.\")"
   ],
   "id": "7240b62a35e4b513",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 12: Batch Process for Rounds or Next Race\n",
    "import os\n",
    "import fastf1\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "BASE_DIR = os.getcwd()\n",
    "log_dir = os.path.join(BASE_DIR, \"logs\")\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(log_dir, f\"batch_process_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"),\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# List of required functions from Cells 2-11\n",
    "required_functions = [\n",
    "    'get_target_event', 'download_practice_data', 'clean_and_aggregate_data',\n",
    "    'compute_driver_performance', 'analyze_telemetry_metrics', 'investigate_sandbagging',\n",
    "    'evaluate_track_evolution', 'analyze_track_characteristics', 'predict_race_outcomes',\n",
    "    'generate_comprehensive_report', 'generate_summary_report'\n",
    "]\n",
    "\n",
    "def batch_process_rounds(year=2024, rounds=None):\n",
    "    # Check if all required functions are defined\n",
    "    missing_functions = [func for func in required_functions if func not in globals()]\n",
    "    if missing_functions:\n",
    "        error_msg = f\"Error: The following functions are not defined: {missing_functions}. Please run Cells 2-11 first.\"\n",
    "        logger.error(error_msg)\n",
    "        print(error_msg)\n",
    "        return {}\n",
    "\n",
    "    fastf1.Cache.enable_cache(os.path.join(BASE_DIR, \"fastf1_cache\"))\n",
    "    fastf1.set_log_level('ERROR')\n",
    "    if rounds is None:\n",
    "        mode = \"next race\"\n",
    "        logger.info(f\"Starting batch process for {year}, Next Race mode\")\n",
    "        print(f\"\\n=== Starting Batch Process for {year}, Next Race ===\")\n",
    "    else:\n",
    "        mode = \"specified rounds\"\n",
    "        logger.info(f\"Starting batch process for {year}, Rounds {list(rounds)}\")\n",
    "        print(f\"\\n=== Starting Batch Process for {year}, Rounds {list(rounds)} ===\")\n",
    "\n",
    "    try:\n",
    "        schedule = fastf1.get_event_schedule(year)\n",
    "        logger.info(f\"Fetched {year} schedule with {len(schedule)} events\")\n",
    "        print(f\"Schedule fetched: {len(schedule)} events\")\n",
    "        display(schedule[['RoundNumber', 'EventName', 'EventDate']])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to fetch event schedule: {e}\")\n",
    "        print(f\"Error fetching event schedule: {e}\")\n",
    "        return {}\n",
    "\n",
    "    results = {}\n",
    "    if rounds is None:\n",
    "        target_info = get_target_event(manual_year=year, manual_round=None, skip_timing_validation=True)\n",
    "        round_number = target_info['round']\n",
    "        rounds_to_process = [round_number]\n",
    "        logger.info(f\"Next race identified: Round {round_number}\")\n",
    "        print(f\"Next race identified: Round {round_number}\")\n",
    "    else:\n",
    "        rounds_to_process = rounds\n",
    "\n",
    "    for round_number in rounds_to_process:\n",
    "        logger.info(f\"Processing Round {round_number}\")\n",
    "        print(f\"\\n=== Processing {year} Round {round_number} ===\")\n",
    "\n",
    "        try:\n",
    "            # Step 2\n",
    "            print(\"Step 2: Fetching target event info...\")\n",
    "            if rounds is not None:\n",
    "                target_info = get_target_event(manual_year=year, manual_round=round_number, skip_timing_validation=True)\n",
    "            event_name = target_info['event_name']\n",
    "            logger.info(f\"Target event: {event_name} (Round {round_number})\")\n",
    "            print(f\"Event: {event_name}\")\n",
    "            print(f\"Target Info: {target_info}\")\n",
    "\n",
    "            # Step 3\n",
    "            print(\"Step 3: Downloading practice data...\")\n",
    "            session_data = download_practice_data(target_info)\n",
    "            if not session_data:\n",
    "                logger.error(f\"No session data for Round {round_number}\")\n",
    "                print(\"Error: No session data retrieved\")\n",
    "                continue\n",
    "            logger.info(f\"Session data keys: {list(session_data.keys())}\")\n",
    "            print(f\"Session data retrieved for: {list(session_data.keys())}\")\n",
    "            for session, data in session_data.items():\n",
    "                print(f\"{session} - Laps: {len(data['laps'])}, Weather: {len(data['weather'])}, Indicators: {len(data['indicators'])}\")\n",
    "\n",
    "            # Step 4\n",
    "            print(\"Step 4: Cleaning and aggregating data...\")\n",
    "            cleaned_data, driver_summary = clean_and_aggregate_data(target_info, session_data)\n",
    "            if cleaned_data == {}:\n",
    "                logger.error(f\"Cleaning failed for Round {round_number}\")\n",
    "                print(\"Error: Cleaning failed, no cleaned data\")\n",
    "                continue\n",
    "            logger.info(f\"Cleaned data keys: {list(cleaned_data.keys())}\")\n",
    "            print(f\"Cleaned data keys: {list(cleaned_data.keys())}\")\n",
    "            print(\"Driver Summary (Top 5):\")\n",
    "            display(driver_summary.head())\n",
    "\n",
    "            # Step 5\n",
    "            print(\"Step 5: Computing driver performance...\")\n",
    "            driver_performance_df = compute_driver_performance(cleaned_data, year, round_number)\n",
    "            if driver_performance_df.empty:\n",
    "                logger.error(f\"Driver performance analysis failed for Round {round_number}\")\n",
    "                print(\"Error: Driver performance DataFrame is empty\")\n",
    "                continue\n",
    "            logger.info(f\"Driver performance rows: {len(driver_performance_df)}\")\n",
    "            print(f\"Driver performance rows: {len(driver_performance_df)}\")\n",
    "            print(\"Driver Performance (Top 5 by FastestLapTime):\")\n",
    "            display(driver_performance_df.sort_values('FastestLapTime').head())\n",
    "\n",
    "            # Step 6\n",
    "            print(\"Step 6: Analyzing telemetry metrics...\")\n",
    "            telemetry_metrics_df = analyze_telemetry_metrics(target_info)\n",
    "            if telemetry_metrics_df.empty:\n",
    "                logger.warning(f\"Telemetry metrics empty for Round {round_number}, proceeding\")\n",
    "                print(\"Warning: Telemetry metrics DataFrame is empty, proceeding\")\n",
    "            logger.info(f\"Telemetry metrics rows: {len(telemetry_metrics_df)}\")\n",
    "            print(f\"Telemetry metrics rows: {len(telemetry_metrics_df)}\")\n",
    "            if not telemetry_metrics_df.empty:\n",
    "                print(\"Telemetry Metrics (Top 5 by MaxSpeed):\")\n",
    "                display(telemetry_metrics_df.sort_values('MaxSpeed', ascending=False).head())\n",
    "\n",
    "            # Step 7\n",
    "            print(\"Step 7: Investigating sandbagging...\")\n",
    "            sandbag_analysis_df = investigate_sandbagging(target_info)\n",
    "            if sandbag_analysis_df.empty:\n",
    "                logger.warning(f\"Sandbagging analysis empty for Round {round_number}, proceeding\")\n",
    "                print(\"Warning: Sandbagging analysis DataFrame is empty, proceeding\")\n",
    "            logger.info(f\"Sandbag analysis rows: {len(sandbag_analysis_df)}\")\n",
    "            print(f\"Sandbag analysis rows: {len(sandbag_analysis_df)}\")\n",
    "            if not sandbag_analysis_df.empty:\n",
    "                print(\"Potential Sandbaggers:\")\n",
    "                display(sandbag_analysis_df[sandbag_analysis_df['SandbagFlag']].sort_values('PredictionDiscrepancy', ascending=False))\n",
    "\n",
    "            # Step 8\n",
    "            print(\"Step 8: Evaluating track evolution...\")\n",
    "            track_evolution_df = evaluate_track_evolution(target_info, cleaned_data)\n",
    "            if track_evolution_df.empty:\n",
    "                logger.warning(f\"Track evolution empty for Round {round_number}, proceeding\")\n",
    "                print(\"Warning: Track evolution DataFrame is empty, proceeding\")\n",
    "            logger.info(f\"Track evolution rows: {len(track_evolution_df)}\")\n",
    "            print(f\"Track evolution rows: {len(track_evolution_df)}\")\n",
    "            if not track_evolution_df.empty:\n",
    "                print(\"Track Evolution:\")\n",
    "                display(track_evolution_df)\n",
    "\n",
    "            # Step 9\n",
    "            print(\"Step 9: Analyzing track characteristics...\")\n",
    "            track_char_drivers_df = analyze_track_characteristics(target_info, cleaned_data)\n",
    "            if track_char_drivers_df.empty:\n",
    "                logger.warning(f\"Track characteristics empty for Round {round_number}, proceeding\")\n",
    "                print(\"Warning: Track characteristics DataFrame is empty, proceeding\")\n",
    "            logger.info(f\"Track characteristics drivers rows: {len(track_char_drivers_df)}\")\n",
    "            print(f\"Track characteristics drivers rows: {len(track_char_drivers_df)}\")\n",
    "            if not track_char_drivers_df.empty:\n",
    "                print(\"Track Characteristics Drivers (Top 5 by FitRank):\")\n",
    "                display(track_char_drivers_df.sort_values('FitRank').head())\n",
    "\n",
    "            # Step 10\n",
    "            print(\"Step 10: Predicting race outcomes...\")\n",
    "            quali_prediction_df, race_prediction_df, fastest_lap_pred = predict_race_outcomes(\n",
    "                target_info, driver_performance_df, telemetry_metrics_df, sandbag_analysis_df,\n",
    "                track_evolution_df, track_char_drivers_df\n",
    "            )\n",
    "            if quali_prediction_df.empty or race_prediction_df.empty:\n",
    "                logger.error(f\"Prediction failed for Round {round_number}\")\n",
    "                print(\"Error: Prediction DataFrames are empty\")\n",
    "                continue\n",
    "            logger.info(f\"Quali prediction rows: {len(quali_prediction_df)}, Race prediction rows: {len(race_prediction_df)}\")\n",
    "            print(f\"Quali prediction rows: {len(quali_prediction_df)}, Race prediction rows: {len(race_prediction_df)}\")\n",
    "            print(\"Qualifying Prediction (Top 5):\")\n",
    "            display(quali_prediction_df.sort_values('QualiPosition').head())\n",
    "            print(\"Race Prediction (Top 10):\")\n",
    "            display(race_prediction_df.sort_values('FinalPosition').head(10))\n",
    "            print(f\"Fastest Lap Prediction: {fastest_lap_pred}\")\n",
    "\n",
    "            # Step 10.5\n",
    "            print(\"Step 10.5: Generating pre-qualifying report...\")\n",
    "            generate_comprehensive_report(\n",
    "                target_info, driver_performance_df, telemetry_metrics_df, sandbag_analysis_df,\n",
    "                track_evolution_df, track_char_drivers_df, quali_prediction_df, race_prediction_df, fastest_lap_pred\n",
    "            )\n",
    "            logger.info(f\"Pre-qualifying report generated for Round {round_number}\")\n",
    "            print(f\"Pre-qualifying report generated: {os.path.join(BASE_DIR, 'reports', str(year), f'R{round_number}', f'{year}_R{round_number}_pre_qualifying_report.pdf')}\")\n",
    "\n",
    "            # Step 11\n",
    "            print(\"Step 11: Generating summary report with actual results...\")\n",
    "            generate_summary_report(target_info, quali_prediction_df, race_prediction_df, fastest_lap_pred)\n",
    "            logger.info(f\"Summary report generated for Round {round_number}\")\n",
    "            print(f\"Summary report saved: {os.path.join(BASE_DIR, 'raw_data', str(year), f'R{round_number}', f'{year}_R{round_number}_summary_results.csv')}\")\n",
    "\n",
    "            results[round_number] = {\n",
    "                'target_info': target_info,\n",
    "                'session_data': session_data,\n",
    "                'cleaned_data': cleaned_data,\n",
    "                'driver_summary': driver_summary,\n",
    "                'driver_performance_df': driver_performance_df,\n",
    "                'telemetry_metrics_df': telemetry_metrics_df,\n",
    "                'sandbag_analysis_df': sandbag_analysis_df,\n",
    "                'track_evolution_df': track_evolution_df,\n",
    "                'track_char_drivers_df': track_char_drivers_df,\n",
    "                'quali_prediction_df': quali_prediction_df,\n",
    "                'race_prediction_df': race_prediction_df,\n",
    "                'fastest_lap_pred': fastest_lap_pred\n",
    "            }\n",
    "            print(f\"Results stored for Round {round_number}: {list(results[round_number].keys())}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing Round {round_number}: {e}\")\n",
    "            print(f\"Error processing Round {round_number}: {e}\")\n",
    "            continue\n",
    "\n",
    "    logger.info(\"Batch processing completed\")\n",
    "    print(\"\\n=== Batch Processing Completed ===\")\n",
    "    print(f\"Processed rounds: {list(results.keys())}\")\n",
    "    return results\n",
    "\n",
    "# Test with Round 4\n",
    "print(\"Starting batch process for Round 4...\")\n",
    "batch_results = batch_process_rounds(year=2024, rounds=range(5,6))\n",
    "\n",
    "# Test with next race (commented out, uncomment to try after fixing)\n",
    "# print(\"Starting batch process for next race...\")\n",
    "# batch_results = batch_process_rounds(year=2025)"
   ],
   "id": "6698118eeec09a00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 13: Consolidate Data into SQLite Database\n",
    "import os\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"raw_data\")\n",
    "DB_PATH = os.path.join(BASE_DIR, \"f1_fantasy.db\")\n",
    "\n",
    "def consolidate_data(rounds=None, force_reimport=False):\n",
    "    print(f\"\\n=== Consolidating Data into {DB_PATH} ===\")\n",
    "    if rounds is None:\n",
    "        print(\"Auto-detecting all rounds from raw_data directory...\")\n",
    "    else:\n",
    "        print(f\"Processing specified rounds: {rounds}\")\n",
    "\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Table schemas (unchanged except for clarity)\n",
    "    tables = {\n",
    "        'events': \"\"\"\n",
    "            CREATE TABLE events (\n",
    "                RoundNumber INTEGER,\n",
    "                Year INTEGER,\n",
    "                EventName TEXT,\n",
    "                EventDate TEXT,\n",
    "                Location TEXT,\n",
    "                TrackLength REAL,\n",
    "                AvgSpeed REAL,\n",
    "                DownforceLevel TEXT,\n",
    "                PRIMARY KEY (Year, RoundNumber)\n",
    "            )\n",
    "        \"\"\",\n",
    "        'driver_performance': \"\"\"\n",
    "            CREATE TABLE driver_performance (\n",
    "                RoundNumber INTEGER,\n",
    "                Year INTEGER,\n",
    "                Driver TEXT,\n",
    "                FastestLapTime REAL,\n",
    "                TheoreticalLapTime REAL,\n",
    "                Sector1Time REAL,\n",
    "                Sector2Time REAL,\n",
    "                Sector3Time REAL,\n",
    "                BasePace REAL,\n",
    "                LapTimeVar REAL,\n",
    "                DegradationSlope REAL,\n",
    "                LapTimeGap REAL,\n",
    "                Team TEXT,\n",
    "                PRIMARY KEY (Year, RoundNumber, Driver)\n",
    "            )\n",
    "        \"\"\",\n",
    "        'telemetry_metrics': \"\"\"\n",
    "            CREATE TABLE telemetry_metrics (\n",
    "                RoundNumber INTEGER,\n",
    "                Year INTEGER,\n",
    "                Driver TEXT,\n",
    "                MaxSpeed REAL,\n",
    "                ThrottleTime REAL,\n",
    "                BrakingIntensity REAL,\n",
    "                Team TEXT,\n",
    "                PRIMARY KEY (Year, RoundNumber, Driver)\n",
    "            )\n",
    "        \"\"\",\n",
    "        'sandbag_analysis': \"\"\"\n",
    "            CREATE TABLE sandbag_analysis (\n",
    "                RoundNumber INTEGER,\n",
    "                Year INTEGER,\n",
    "                Driver TEXT,\n",
    "                FastestLapTime REAL,\n",
    "                MaxSpeed REAL,\n",
    "                ExpectedLapTime REAL,\n",
    "                LapTimeDiscrepancy REAL,\n",
    "                PredictedLapTime REAL,\n",
    "                PredictionDiscrepancy REAL,\n",
    "                SpeedRank REAL,\n",
    "                LapTimeRank REAL,\n",
    "                SandbagFlag INTEGER,\n",
    "                ThrottleTime REAL,\n",
    "                Team TEXT,\n",
    "                PRIMARY KEY (Year, RoundNumber, Driver)\n",
    "            )\n",
    "        \"\"\",\n",
    "        'track_evolution': \"\"\"\n",
    "            CREATE TABLE track_evolution (\n",
    "                RoundNumber INTEGER,\n",
    "                Year INTEGER,\n",
    "                Session TEXT,\n",
    "                AvgLapTime REAL,\n",
    "                StdLapTime REAL,\n",
    "                LapCount INTEGER,\n",
    "                PRIMARY KEY (Year, RoundNumber, Session)\n",
    "            )\n",
    "        \"\"\",\n",
    "        'track_characteristics': \"\"\"\n",
    "            CREATE TABLE track_characteristics (\n",
    "                RoundNumber INTEGER,\n",
    "                Year INTEGER,\n",
    "                Driver TEXT,\n",
    "                SpeedST REAL,\n",
    "                Sector1Time REAL,\n",
    "                Sector2Time REAL,\n",
    "                Sector3Time REAL,\n",
    "                LapTime REAL,\n",
    "                StraightSpeedPotential REAL,\n",
    "                CorneringPotential REAL,\n",
    "                AvgSpeed REAL,\n",
    "                OverallFit REAL,\n",
    "                FitRank REAL,\n",
    "                StraightZScore REAL,\n",
    "                CorneringZScore REAL,\n",
    "                Team TEXT,\n",
    "                PRIMARY KEY (Year, RoundNumber, Driver)\n",
    "            )\n",
    "        \"\"\",\n",
    "        'predictions': \"\"\"\n",
    "            CREATE TABLE predictions (\n",
    "                RoundNumber INTEGER,\n",
    "                Year INTEGER,\n",
    "                Driver TEXT,\n",
    "                QualiAdjustedLapTime REAL,\n",
    "                QualiPosition INTEGER,\n",
    "                RaceAdjustedRaceTime REAL,\n",
    "                RaceFinalPosition INTEGER,\n",
    "                FastestLapPred TEXT,\n",
    "                Team TEXT,\n",
    "                PRIMARY KEY (Year, RoundNumber, Driver)\n",
    "            )\n",
    "        \"\"\",\n",
    "        'results': \"\"\"\n",
    "            CREATE TABLE results (\n",
    "                RoundNumber INTEGER,\n",
    "                Year INTEGER,\n",
    "                Driver TEXT,\n",
    "                GridPosition INTEGER,\n",
    "                FinalPosition INTEGER,\n",
    "                BasePace REAL,\n",
    "                DegradationSlope REAL,\n",
    "                ActualRacePace REAL,\n",
    "                ActualDegradationSlope REAL,\n",
    "                ActualOvertakes REAL,\n",
    "                NumPitStops REAL,\n",
    "                TotalPitTime REAL,\n",
    "                AvgPitTime REAL,\n",
    "                PaceError REAL,\n",
    "                DegError REAL,\n",
    "                PositionError REAL,\n",
    "                PitStopCountError REAL,\n",
    "                PitTimeError REAL,\n",
    "                FuelPenalty REAL,\n",
    "                TrafficPenalty REAL,\n",
    "                CompoundStrategy TEXT,\n",
    "                StintLengths TEXT,\n",
    "                ActualTyreStrategy TEXT,\n",
    "                TotalPoints REAL,\n",
    "                Team TEXT,\n",
    "                PRIMARY KEY (Year, RoundNumber, Driver)\n",
    "            )\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "    # Drop tables if force_reimport is True\n",
    "    if force_reimport:\n",
    "        for table_name in tables.keys():\n",
    "            cursor.execute(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "            print(f\"Dropped table: {table_name}\")\n",
    "\n",
    "    # Create tables\n",
    "    for table_name, create_stmt in tables.items():\n",
    "        cursor.execute(create_stmt)\n",
    "        print(f\"Created table: {table_name}\")\n",
    "\n",
    "    # Get existing rounds (empty after drop, kept for non-force runs)\n",
    "    existing_rounds = pd.read_sql_query(\"SELECT Year, RoundNumber FROM events\", conn)\n",
    "    existing_rounds_set = set(zip(existing_rounds['Year'], existing_rounds['RoundNumber']))\n",
    "\n",
    "    # Auto-detect rounds if not specified\n",
    "    if rounds is None:\n",
    "        years = [y for y in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, y)) and y.isdigit()]\n",
    "        print(f\"Detected years: {years}\")\n",
    "        rounds_to_process = []\n",
    "        for year in years:\n",
    "            year_dir = os.path.join(DATA_DIR, year)\n",
    "            round_dirs = [d for d in os.listdir(year_dir) if d.startswith('R') and os.path.isdir(os.path.join(year_dir, d))]\n",
    "            for round_dir in round_dirs:\n",
    "                round_number = int(round_dir.replace('R', ''))\n",
    "                rounds_to_process.append((int(year), round_number))\n",
    "    else:\n",
    "        rounds_to_process = [(2024, r) for r in rounds]\n",
    "\n",
    "    for year, round_number in rounds_to_process:\n",
    "        if (year, round_number) in existing_rounds_set and not force_reimport:\n",
    "            print(f\"Skipping Round {round_number} of {year} - already in database\")\n",
    "            continue\n",
    "\n",
    "        round_dir = os.path.join(DATA_DIR, str(year), f\"R{round_number}\")\n",
    "        if not os.path.exists(round_dir):\n",
    "            print(f\"Warning: Directory {round_dir} not found, skipping\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing Round {round_number} of {year}...\")\n",
    "\n",
    "        # Events\n",
    "        event_data = {\n",
    "            'RoundNumber': round_number,\n",
    "            'Year': year,\n",
    "            'EventName': f\"Round {round_number}\",\n",
    "            'EventDate': None,\n",
    "            'Location': None,\n",
    "            'TrackLength': None,\n",
    "            'AvgSpeed': None,\n",
    "            'DownforceLevel': None\n",
    "        }\n",
    "        try:\n",
    "            track_char_df = pd.read_csv(os.path.join(round_dir, f\"{year}_R{round_number}_track_characteristics.csv\"))\n",
    "            event_subset = track_char_df.iloc[0][['Track', 'LapLength', 'DownforceRequirement']].to_dict()\n",
    "            event_data.update({\n",
    "                'EventName': event_subset['Track'],\n",
    "                'TrackLength': event_subset['LapLength'],\n",
    "                'DownforceLevel': event_subset['DownforceRequirement']\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not enrich event data for Round {round_number} of {year}: {e}\")\n",
    "        pd.DataFrame([event_data]).to_sql('events', conn, if_exists='append', index=False)\n",
    "        print(f\"Loaded events: 1 row\")\n",
    "\n",
    "        # Driver Performance\n",
    "        try:\n",
    "            driver_performance_df = pd.read_csv(os.path.join(round_dir, f\"{year}_R{round_number}_driver_performance.csv\"))\n",
    "            driver_performance_df['RoundNumber'] = round_number\n",
    "            driver_performance_df['Year'] = year\n",
    "            driver_performance_df.to_sql('driver_performance', conn, if_exists='append', index=False)\n",
    "            print(f\"Loaded driver_performance: {len(driver_performance_df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading driver_performance for Round {round_number} of {year}: {e}\")\n",
    "\n",
    "        # Telemetry Metrics\n",
    "        try:\n",
    "            telemetry_metrics_df = pd.read_csv(os.path.join(round_dir, f\"{year}_R{round_number}_telemetry_metrics.csv\"))\n",
    "            telemetry_metrics_df['RoundNumber'] = round_number\n",
    "            telemetry_metrics_df['Year'] = year\n",
    "            telemetry_metrics_df.to_sql('telemetry_metrics', conn, if_exists='append', index=False)\n",
    "            print(f\"Loaded telemetry_metrics: {len(telemetry_metrics_df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading telemetry_metrics for Round {round_number} of {year}: {e}\")\n",
    "\n",
    "        # Sandbag Analysis\n",
    "        try:\n",
    "            sandbag_analysis_df = pd.read_csv(os.path.join(round_dir, f\"{year}_R{round_number}_sandbag_analysis.csv\"))\n",
    "            sandbag_analysis_df['RoundNumber'] = round_number\n",
    "            sandbag_analysis_df['Year'] = year\n",
    "            sandbag_analysis_df.to_sql('sandbag_analysis', conn, if_exists='append', index=False)\n",
    "            print(f\"Loaded sandbag_analysis: {len(sandbag_analysis_df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sandbag_analysis for Round {round_number} of {year}: {e}\")\n",
    "\n",
    "        # Track Evolution\n",
    "        try:\n",
    "            track_evolution_df = pd.read_csv(os.path.join(round_dir, f\"{year}_R{round_number}_track_evolution.csv\"))\n",
    "            track_evolution_df['RoundNumber'] = round_number\n",
    "            track_evolution_df['Year'] = year\n",
    "            track_evolution_df.to_sql('track_evolution', conn, if_exists='append', index=False)\n",
    "            print(f\"Loaded track_evolution: {len(track_evolution_df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading track_evolution for Round {round_number} of {year}: {e}\")\n",
    "\n",
    "        # Track Characteristics\n",
    "        try:\n",
    "            track_char_drivers_df = pd.read_csv(os.path.join(round_dir, f\"{year}_R{round_number}_track_characteristics_drivers.csv\"))\n",
    "            track_char_drivers_df['RoundNumber'] = round_number\n",
    "            track_char_drivers_df['Year'] = year\n",
    "            track_char_drivers_df.to_sql('track_characteristics', conn, if_exists='append', index=False)\n",
    "            print(f\"Loaded track_characteristics: {len(track_char_drivers_df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading track_characteristics for Round {round_number} of {year}: {e}\")\n",
    "\n",
    "        # Predictions\n",
    "        try:\n",
    "            quali_pred_df = pd.read_csv(os.path.join(round_dir, f\"{year}_R{round_number}_quali_prediction.csv\"))\n",
    "            race_pred_df = pd.read_csv(os.path.join(round_dir, f\"{year}_R{round_number}_race_prediction.csv\"))\n",
    "            print(f\"Quali columns: {quali_pred_df.columns.tolist()}\")\n",
    "            print(f\"Race columns: {race_pred_df.columns.tolist()}\")\n",
    "            predictions_df = quali_pred_df[['Driver', 'AdjustedLapTime', 'QualiPosition', 'Team']].merge(\n",
    "                race_pred_df[['Driver', 'AdjustedRaceTime', 'FinalPosition']], on='Driver', how='outer'\n",
    "            )\n",
    "            predictions_df['RoundNumber'] = round_number\n",
    "            predictions_df['Year'] = year\n",
    "            predictions_df['FastestLapPred'] = None  # Placeholder\n",
    "            predictions_df.rename(columns={\n",
    "                'AdjustedLapTime': 'QualiAdjustedLapTime',\n",
    "                'AdjustedRaceTime': 'RaceAdjustedRaceTime',  # Fixed renaming\n",
    "                'FinalPosition': 'RaceFinalPosition'\n",
    "            }, inplace=True)\n",
    "            print(f\"Predictions columns after merge: {predictions_df.columns.tolist()}\")\n",
    "            predictions_df.to_sql('predictions', conn, if_exists='append', index=False)\n",
    "            print(f\"Loaded predictions: {len(predictions_df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading predictions for Round {round_number} of {year}: {e}\")\n",
    "\n",
    "        # Results\n",
    "        try:\n",
    "            results_df = pd.read_csv(os.path.join(round_dir, f\"{year}_R{round_number}_summary_results.csv\"))\n",
    "            results_df['RoundNumber'] = round_number\n",
    "            results_df['Year'] = year\n",
    "            results_df['TotalPoints'] = None  # Placeholder\n",
    "            results_df.to_sql('results', conn, if_exists='append', index=False)\n",
    "            print(f\"Loaded results: {len(results_df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading results for Round {round_number} of {year}: {e}\")\n",
    "\n",
    "    # Verify table contents\n",
    "    for table in tables.keys():\n",
    "        count = pd.read_sql_query(f\"SELECT COUNT(*) as count FROM {table}\", conn).iloc[0]['count']\n",
    "        print(f\"Total rows in {table}: {count}\")\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(f\"\\nData consolidation completed. Database saved at {DB_PATH}\")\n",
    "\n",
    "# Run with force reimport\n",
    "consolidate_data(force_reimport=True)"
   ],
   "id": "cf0a3f884e7f8c17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fe33d2ecc4766d39",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
